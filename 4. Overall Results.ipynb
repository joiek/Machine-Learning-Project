{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c2baadc",
   "metadata": {},
   "source": [
    "## 4. Overall Results\n",
    "\n",
    "Final analysis of best models for each featuriser to determine best featurisation method for Tox21."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b8c2bcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements\n",
    "import deepchem as dc\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, losses\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import (precision_recall_curve, average_precision_score, PrecisionRecallDisplay)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142edd40",
   "metadata": {},
   "source": [
    "Tox21 Assay List: <br>\n",
    "<ol>\n",
    "    <li> NR-AR </li>\n",
    "    <li> NR-AR-LBD </li>\n",
    "    <li> NR-AhR </li>\n",
    "    <li> NR-Aromatase </li>\n",
    "    <li> NR-ER </li>\n",
    "    <li> NR-ER-LBD </li>\n",
    "    <li> NR-PPAR-gamma </li>\n",
    "    <li> SR-ARE </li>\n",
    "    <li> SR-ATAD5 </li>\n",
    "    <li> SR-HSE </li>\n",
    "    <li> SR-MMP </li>\n",
    "    <li> SR-p53 </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bbccf386",
   "metadata": {},
   "outputs": [],
   "source": [
    "assays = ['NR-AR', 'NR-AR-LBD','NR-AhR', 'NR-Aromatase', 'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma',\n",
    "         'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec35956",
   "metadata": {},
   "source": [
    "## SmilesToImage Featuriser - CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9af0aeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_smiles, datasets_smiles, transformers_smiles = dc.molnet.load_tox21(\n",
    "    featurizer = dc.feat.SmilesToImage(img_size=80, img_spec='std'),\n",
    "    save_dir=r'C:\\Users\\ym20201\\Documents\\Datasets',\n",
    "    data_dir=r'C:\\Users\\ym20201\\Documents\\Datasets')\n",
    "\n",
    "splitter = dc.splits.RandomSplitter()\n",
    "\n",
    "train_data_smiles, valid_data_smiles, test_data_smiles = datasets_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b322712d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SmilesToImage Model 7\n",
    "smiles2img_model = dc.models.CNN(\n",
    "    n_tasks = len(tasks_smiles), # Num of tasks, i.e. width of y\n",
    "    n_features=len(train_data_smiles.X[2]), # number of features, i.e. width of X\n",
    "    dims=1,\n",
    "    layer_filter=[500,500,200],\n",
    "    mode='classification',\n",
    "    weight_init_stddevs=0.02, \n",
    "    bias_init_consts=1.0,\n",
    "    dropouts=0.5,\n",
    "    dense_layer_size=[500,200,100],\n",
    "    activation_fns=['relu'],\n",
    "    uncertainty=False,\n",
    "    pool_type='max',\n",
    "    residual=False,\n",
    "    padding='valid') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "ad77a462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7096834369734222"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles2img_model.fit(\n",
    "    train_data_smiles,\n",
    "    nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "18ff7c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles2img_pred = smiles2img_model.predict(test_data_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5978ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for accessing y_true and y_pred of each assay\n",
    "def y_true(assay_num, test_data):\n",
    "    y_true = []\n",
    "    for i in range(len(test_data.y)):\n",
    "        y_true.append(test_data.y[i][assay_num - 1])\n",
    "        \n",
    "    return y_true\n",
    "\n",
    "def y_pred(assay_num, pred_data):\n",
    "    y_pred = []\n",
    "    for i in range(len(pred_data)):\n",
    "        y_pred.append( pred_data[i][assay_num - 1][0])\n",
    "#         y_pred = pred_data[i][assay_num - 1] #not sure which one\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b0916450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_true of each assay\n",
    "smiles2img_y_true_1 = y_true(1, test_data_smiles)\n",
    "smiles2img_y_true_2 = y_true(2, test_data_smiles)\n",
    "smiles2img_y_true_3 = y_true(3, test_data_smiles)\n",
    "smiles2img_y_true_4 = y_true(4, test_data_smiles)\n",
    "smiles2img_y_true_5 = y_true(5, test_data_smiles)\n",
    "smiles2img_y_true_6 = y_true(6, test_data_smiles)\n",
    "smiles2img_y_true_7 = y_true(7, test_data_smiles)\n",
    "smiles2img_y_true_8 = y_true(8, test_data_smiles)\n",
    "smiles2img_y_true_9 = y_true(9, test_data_smiles)\n",
    "smiles2img_y_true_10 = y_true(10, test_data_smiles)\n",
    "smiles2img_y_true_11 = y_true(11, test_data_smiles)\n",
    "smiles2img_y_true_12 = y_true(12, test_data_smiles)\n",
    "\n",
    "smiles2img_y_true = [smiles2img_y_true_1, smiles2img_y_true_2, smiles2img_y_true_3, smiles2img_y_true_4, smiles2img_y_true_5, \n",
    "        smiles2img_y_true_6, smiles2img_y_true_7, smiles2img_y_true_8, smiles2img_y_true_9, smiles2img_y_true_10,\n",
    "        smiles2img_y_true_11, smiles2img_y_true_12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "3441d9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred of each assay\n",
    "smiles2img_y_pred_1 = y_pred(1, smiles2img_pred)\n",
    "smiles2img_y_pred_2 = y_pred(2, smiles2img_pred)\n",
    "smiles2img_y_pred_3 = y_pred(3, smiles2img_pred)\n",
    "smiles2img_y_pred_4 = y_pred(4, smiles2img_pred)\n",
    "smiles2img_y_pred_5 = y_pred(5, smiles2img_pred)\n",
    "smiles2img_y_pred_6 = y_pred(6, smiles2img_pred)\n",
    "smiles2img_y_pred_7 = y_pred(7, smiles2img_pred)\n",
    "smiles2img_y_pred_8 = y_pred(8, smiles2img_pred)\n",
    "smiles2img_y_pred_9 = y_pred(9, smiles2img_pred)\n",
    "smiles2img_y_pred_10 = y_pred(10, smiles2img_pred)\n",
    "smiles2img_y_pred_11 = y_pred(11, smiles2img_pred)\n",
    "smiles2img_y_pred_12 = y_pred(12, smiles2img_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "538d1bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rounding predicted outputs to discrete values\n",
    "def round_pred(y_pred):\n",
    "    y_pred_new = []\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] < 0.5: #setting threshold as 0.5\n",
    "            new = 0\n",
    "        else:\n",
    "            new = 1\n",
    "        y_pred_new.append(new)\n",
    "    return y_pred_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "4c1e187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rounding predicted probabilities to binary values\n",
    "smiles2img_y_pred_new_1 = round_pred(smiles2img_y_pred_1)\n",
    "smiles2img_y_pred_new_2 = round_pred(smiles2img_y_pred_2)\n",
    "smiles2img_y_pred_new_3 = round_pred(smiles2img_y_pred_3)\n",
    "smiles2img_y_pred_new_4 = round_pred(smiles2img_y_pred_4)\n",
    "smiles2img_y_pred_new_5 = round_pred(smiles2img_y_pred_5)\n",
    "smiles2img_y_pred_new_6 = round_pred(smiles2img_y_pred_6)\n",
    "smiles2img_y_pred_new_7= round_pred(smiles2img_y_pred_7)\n",
    "smiles2img_y_pred_new_8 = round_pred(smiles2img_y_pred_8)\n",
    "smiles2img_y_pred_new_9 = round_pred(smiles2img_y_pred_9)\n",
    "smiles2img_y_pred_new_10 = round_pred(smiles2img_y_pred_10)\n",
    "smiles2img_y_pred_new_11 = round_pred(smiles2img_y_pred_11)\n",
    "smiles2img_y_pred_new_12 = round_pred(smiles2img_y_pred_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "a02a31ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the values into arrays\n",
    "smiles2img_y_pred = [smiles2img_y_pred_1, smiles2img_y_pred_2, smiles2img_y_pred_3, smiles2img_y_pred_4, smiles2img_y_pred_5, \n",
    "             smiles2img_y_pred_6, smiles2img_y_pred_7, smiles2img_y_pred_8, smiles2img_y_pred_9, smiles2img_y_pred_10,\n",
    "            smiles2img_y_pred_11, smiles2img_y_pred_12]\n",
    "\n",
    "smiles2img_y_pred_new = [smiles2img_y_pred_new_1, smiles2img_y_pred_new_2, smiles2img_y_pred_new_3, smiles2img_y_pred_new_4, smiles2img_y_pred_new_5, \n",
    "             smiles2img_y_pred_new_6, smiles2img_y_pred_new_7, smiles2img_y_pred_new_8, smiles2img_y_pred_new_9, smiles2img_y_pred_new_10,\n",
    "            smiles2img_y_pred_new_11, smiles2img_y_pred_new_12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66248f55",
   "metadata": {},
   "source": [
    "## ECFP Featuriser - MultitaskClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "f8fe1764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CircularFingerprint (ECFP) featuriser\n",
    "tasks_ecfp, datasets_ecfp, transformers_ecfp = dc.molnet.load_tox21(\n",
    "    featurizer=dc.feat.CircularFingerprint(),\n",
    "    save_dir=r'C:\\Users\\ym20201\\Documents\\Datasets',\n",
    "    data_dir=r'C:\\Users\\ym20201\\Documents\\Datasets')\n",
    "\n",
    "splitter = dc.splits.RandomSplitter()\n",
    "\n",
    "train_data_ecfp, valid_data_ecfp,test_data_ecfp = datasets_ecfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "23f4c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ECFP Model 5\n",
    "ecfp_model = dc.models.RobustMultitaskClassifier(\n",
    "    n_tasks = len(tasks_ecfp),\n",
    "    n_features = len(valid_data_ecfp.X[3]),\n",
    "    layer_sizes=[500,500,200],\n",
    "    weight_init_stddevs=0.02, \n",
    "    bias_init_consts=1.0,\n",
    "    weight_decay_penalty=0.0,\n",
    "    weight_decay_penalty_type='12',\n",
    "    dropouts=[0.8,0.5,0.0],\n",
    "    activation_fns=['relu'],  \n",
    "    n_classes=12,\n",
    "    learning_rate=0.01,\n",
    "    batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "6b933d0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27393007278442383"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecfp_model.fit(train_data_ecfp,\n",
    "              nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "53ca8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecfp_pred = ecfp_model.predict(test_data_ecfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "278dcec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_true of each assay\n",
    "ecfp_y_true_1 = y_true(1, test_data_ecfp)\n",
    "ecfp_y_true_2 = y_true(2, test_data_ecfp)\n",
    "ecfp_y_true_3 = y_true(3, test_data_ecfp)\n",
    "ecfp_y_true_4 = y_true(4, test_data_ecfp)\n",
    "ecfp_y_true_5 = y_true(5, test_data_ecfp)\n",
    "ecfp_y_true_6 = y_true(6, test_data_ecfp)\n",
    "ecfp_y_true_7 = y_true(7, test_data_ecfp)\n",
    "ecfp_y_true_8 = y_true(8, test_data_ecfp)\n",
    "ecfp_y_true_9 = y_true(9, test_data_ecfp)\n",
    "ecfp_y_true_10 = y_true(10, test_data_ecfp)\n",
    "ecfp_y_true_11 = y_true(11, test_data_ecfp)\n",
    "ecfp_y_true_12 = y_true(12, test_data_ecfp)\n",
    "\n",
    "ecfp_y_true = [ecfp_y_true_1, ecfp_y_true_2, ecfp_y_true_3, ecfp_y_true_4, ecfp_y_true_5, \n",
    "        ecfp_y_true_6, ecfp_y_true_7, ecfp_y_true_8, ecfp_y_true_9, ecfp_y_true_10,\n",
    "        ecfp_y_true_11, ecfp_y_true_12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f080854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred of each assay\n",
    "ecfp_y_pred_1 = y_pred(1, ecfp_pred)\n",
    "ecfp_y_pred_2 = y_pred(2, ecfp_pred)\n",
    "ecfp_y_pred_3 = y_pred(3, ecfp_pred)\n",
    "ecfp_y_pred_4 = y_pred(4, ecfp_pred)\n",
    "ecfp_y_pred_5 = y_pred(5, ecfp_pred)\n",
    "ecfp_y_pred_6 = y_pred(6, ecfp_pred)\n",
    "ecfp_y_pred_7 = y_pred(7, ecfp_pred)\n",
    "ecfp_y_pred_8 = y_pred(8, ecfp_pred)\n",
    "ecfp_y_pred_9 = y_pred(9, ecfp_pred)\n",
    "ecfp_y_pred_10 = y_pred(10, ecfp_pred)\n",
    "ecfp_y_pred_11 = y_pred(11, ecfp_pred)\n",
    "ecfp_y_pred_12 = y_pred(12, ecfp_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "fd08319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rounding predicted probabilities to binary values\n",
    "ecfp_y_pred_new_1 = round_pred(ecfp_y_pred_1)\n",
    "ecfp_y_pred_new_2 = round_pred(ecfp_y_pred_2)\n",
    "ecfp_y_pred_new_3 = round_pred(ecfp_y_pred_3)\n",
    "ecfp_y_pred_new_4 = round_pred(ecfp_y_pred_4)\n",
    "ecfp_y_pred_new_5 = round_pred(ecfp_y_pred_5)\n",
    "ecfp_y_pred_new_6 = round_pred(ecfp_y_pred_6)\n",
    "ecfp_y_pred_new_7= round_pred(ecfp_y_pred_7)\n",
    "ecfp_y_pred_new_8 = round_pred(ecfp_y_pred_8)\n",
    "ecfp_y_pred_new_9 = round_pred(ecfp_y_pred_9)\n",
    "ecfp_y_pred_new_10 = round_pred(ecfp_y_pred_10)\n",
    "ecfp_y_pred_new_11 = round_pred(ecfp_y_pred_11)\n",
    "ecfp_y_pred_new_12 = round_pred(ecfp_y_pred_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "656a879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the values into arrays\n",
    "ecfp_y_pred = [ecfp_y_pred_1, ecfp_y_pred_2, ecfp_y_pred_3, ecfp_y_pred_4, ecfp_y_pred_5, \n",
    "             ecfp_y_pred_6, ecfp_y_pred_7, ecfp_y_pred_8, ecfp_y_pred_9, ecfp_y_pred_10,\n",
    "            ecfp_y_pred_11, ecfp_y_pred_12]\n",
    "\n",
    "ecfp_y_pred_new = [ecfp_y_pred_new_1, ecfp_y_pred_new_2, ecfp_y_pred_new_3, ecfp_y_pred_new_4, ecfp_y_pred_new_5, \n",
    "             ecfp_y_pred_new_6, ecfp_y_pred_new_7, ecfp_y_pred_new_8, ecfp_y_pred_new_9, ecfp_y_pred_new_10,\n",
    "            ecfp_y_pred_new_11, ecfp_y_pred_new_12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587efeb7",
   "metadata": {},
   "source": [
    "## ConvMol Featuriser - GraphConv Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f082cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GraphConv featuriser\n",
    "tasks_convmol, datasets_convmol, transformers_convmol = dc.molnet.load_tox21(\n",
    "    featurizer = dc.feat.ConvMolFeaturizer(),\n",
    "    save_dir=r'C:\\Users\\ym20201\\Documents\\Datasets',\n",
    "    data_dir=r'C:\\Users\\ym20201\\Documents\\Datasets')\n",
    "\n",
    "splitter = dc.splits.RandomSplitter()\n",
    "\n",
    "train_data_convmol, valid_data_convmol,test_data_convmol = datasets_convmol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "72f20486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 6\n",
    "convmol_model = dc.models.GraphConvModel(\n",
    "    n_tasks = len(tasks_convmol),\n",
    "    graph_conv_layers=[32,32,32],\n",
    "    dense_layer_size=128,\n",
    "    dropout=0.0,\n",
    "    mode='classification',\n",
    "    number_atom_features=75,#default value\n",
    "    batch_normalize=True,\n",
    "    uncertainty=False,\n",
    "    n_classes=12,\n",
    "    learning_rate=0.01,\n",
    "    batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "2d4052ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_14:0\", shape=(443,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_13:0\", shape=(443, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Cast_4:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_17:0\", shape=(1364,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_16:0\", shape=(1364, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Cast_5:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_20:0\", shape=(1323,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_19:0\", shape=(1323, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Cast_6:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_23:0\", shape=(136,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_22:0\", shape=(136, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Cast_7:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_11:0\", shape=(443,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_10:0\", shape=(443, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_13:0\", shape=(1364,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_12:0\", shape=(1364, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Cast_1:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_15:0\", shape=(1323,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_14:0\", shape=(1323, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Cast_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_17:0\", shape=(136,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_16:0\", shape=(136, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Cast_3:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_19:0\", shape=(0,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_18:0\", shape=(0, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Cast_4:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_21:0\", shape=(0,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_20:0\", shape=(0, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Cast_5:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_23:0\", shape=(0,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_22:0\", shape=(0, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Cast_6:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_25:0\", shape=(0,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_24:0\", shape=(0, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Cast_7:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_27:0\", shape=(0,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_26:0\", shape=(0, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Cast_8:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_29:0\", shape=(0,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_28:0\", shape=(0, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Cast_9:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_14:0\", shape=(443,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_13:0\", shape=(443, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Cast_4:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_17:0\", shape=(1364,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_16:0\", shape=(1364, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Cast_5:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_20:0\", shape=(1323,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_19:0\", shape=(1323, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Cast_6:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_23:0\", shape=(136,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_22:0\", shape=(136, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Cast_7:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_11:0\", shape=(443,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_10:0\", shape=(443, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_13:0\", shape=(1364,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_12:0\", shape=(1364, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Cast_1:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_15:0\", shape=(1323,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_14:0\", shape=(1323, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Cast_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_17:0\", shape=(136,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_16:0\", shape=(136, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Cast_3:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_19:0\", shape=(0,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_18:0\", shape=(0, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Cast_4:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_21:0\", shape=(0,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_20:0\", shape=(0, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Cast_5:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_23:0\", shape=(0,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_22:0\", shape=(0, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Cast_6:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_25:0\", shape=(0,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_24:0\", shape=(0, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Cast_7:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_27:0\", shape=(0,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_26:0\", shape=(0, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Cast_8:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_29:0\", shape=(0,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_28:0\", shape=(0, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Cast_9:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_14:0\", shape=(443,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_13:0\", shape=(443, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Cast_4:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_17:0\", shape=(1364,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_16:0\", shape=(1364, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Cast_5:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_20:0\", shape=(1323,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_19:0\", shape=(1323, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Cast_6:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_23:0\", shape=(136,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_22:0\", shape=(136, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Cast_7:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_14:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_13:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Cast_4:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_17:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_16:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Cast_5:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_20:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_19:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Cast_6:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_23:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_22:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Cast_7:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_11:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_10:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_13:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_12:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Cast_1:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_15:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_14:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Cast_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_17:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_16:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Cast_3:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_14:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_13:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Cast_4:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_17:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_16:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Cast_5:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_20:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_19:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Cast_6:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_23:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_22:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Cast_7:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_11:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_10:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_13:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_12:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Cast_1:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_15:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_14:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Cast_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_17:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_16:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Cast_3:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_14:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_13:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Cast_4:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_17:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_16:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Cast_5:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_20:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_19:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Cast_6:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_23:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_22:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Cast_7:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_26:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_25:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Cast_8:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_29:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_28:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Cast_9:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_21:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_20:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Cast_5:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_26:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_25:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Cast_8:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_29:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_28:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Cast_9:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_21:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_20:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Cast_5:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_26:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_25:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Cast_8:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_29:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_28:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Cast_9:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_32:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_31:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Cast_10:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_35:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Reshape_34:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_8/Cast_11:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_19:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Reshape_18:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_8/Cast_4:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_32:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_31:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Cast_10:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_35:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Reshape_34:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_7/Cast_11:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_19:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Reshape_18:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_conv_7/Cast_4:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_32:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_31:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Cast_10:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "/Users/Joie/.local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_35:0\", shape=(None,), dtype=int32), values=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Reshape_34:0\", shape=(None, 32), dtype=float32), dense_shape=Tensor(\"gradient_tape/private__graph_conv_keras_model_2/graph_pool_6/Cast_11:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8521888097127278"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convmol_model.fit(\n",
    "    train_data_convmol,\n",
    "    nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "eeb5566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "convmol_pred = convmol_model.predict(test_data_convmol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "5fff5ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_true of each assay\n",
    "convmol_y_true_1 = y_true(1, test_data_convmol)\n",
    "convmol_y_true_2 = y_true(2, test_data_convmol)\n",
    "convmol_y_true_3 = y_true(3, test_data_convmol)\n",
    "convmol_y_true_4 = y_true(4, test_data_convmol)\n",
    "convmol_y_true_5 = y_true(5, test_data_convmol)\n",
    "convmol_y_true_6 = y_true(6, test_data_convmol)\n",
    "convmol_y_true_7 = y_true(7, test_data_convmol)\n",
    "convmol_y_true_8 = y_true(8, test_data_convmol)\n",
    "convmol_y_true_9 = y_true(9, test_data_convmol)\n",
    "convmol_y_true_10 = y_true(10, test_data_convmol)\n",
    "convmol_y_true_11 = y_true(11, test_data_convmol)\n",
    "convmol_y_true_12 = y_true(12, test_data_convmol)\n",
    "\n",
    "convmol_y_true = [convmol_y_true_1, convmol_y_true_2, convmol_y_true_3, convmol_y_true_4, convmol_y_true_5, \n",
    "        convmol_y_true_6, convmol_y_true_7, convmol_y_true_8, convmol_y_true_9, convmol_y_true_10,\n",
    "        convmol_y_true_11, convmol_y_true_12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "88a21b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred of each assay\n",
    "convmol_y_pred_1 = y_pred(1, convmol_pred)\n",
    "convmol_y_pred_2 = y_pred(2, convmol_pred)\n",
    "convmol_y_pred_3 = y_pred(3, convmol_pred)\n",
    "convmol_y_pred_4 = y_pred(4, convmol_pred)\n",
    "convmol_y_pred_5 = y_pred(5, convmol_pred)\n",
    "convmol_y_pred_6 = y_pred(6, convmol_pred)\n",
    "convmol_y_pred_7 = y_pred(7, convmol_pred)\n",
    "convmol_y_pred_8 = y_pred(8, convmol_pred)\n",
    "convmol_y_pred_9 = y_pred(9, convmol_pred)\n",
    "convmol_y_pred_10 = y_pred(10, convmol_pred)\n",
    "convmol_y_pred_11 = y_pred(11, convmol_pred)\n",
    "convmol_y_pred_12 = y_pred(12, convmol_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e34fd3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rounding predicted probabilities to binary values\n",
    "convmol_y_pred_new_1 = round_pred(convmol_y_pred_1)\n",
    "convmol_y_pred_new_2 = round_pred(convmol_y_pred_2)\n",
    "convmol_y_pred_new_3 = round_pred(convmol_y_pred_3)\n",
    "convmol_y_pred_new_4 = round_pred(convmol_y_pred_4)\n",
    "convmol_y_pred_new_5 = round_pred(convmol_y_pred_5)\n",
    "convmol_y_pred_new_6 = round_pred(convmol_y_pred_6)\n",
    "convmol_y_pred_new_7= round_pred(convmol_y_pred_7)\n",
    "convmol_y_pred_new_8 = round_pred(convmol_y_pred_8)\n",
    "convmol_y_pred_new_9 = round_pred(convmol_y_pred_9)\n",
    "convmol_y_pred_new_10 = round_pred(convmol_y_pred_10)\n",
    "convmol_y_pred_new_11 = round_pred(convmol_y_pred_11)\n",
    "convmol_y_pred_new_12 = round_pred(convmol_y_pred_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fd003dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the values into arrays\n",
    "convmol_y_pred = [convmol_y_pred_1, convmol_y_pred_2, convmol_y_pred_3, convmol_y_pred_4, convmol_y_pred_5, \n",
    "             convmol_y_pred_6, convmol_y_pred_7, convmol_y_pred_8, convmol_y_pred_9, convmol_y_pred_10,\n",
    "            convmol_y_pred_11, convmol_y_pred_12]\n",
    "\n",
    "convmol_y_pred_new = [convmol_y_pred_new_1, convmol_y_pred_new_2, convmol_y_pred_new_3, convmol_y_pred_new_4, convmol_y_pred_new_5, \n",
    "             convmol_y_pred_new_6, convmol_y_pred_new_7, convmol_y_pred_new_8, convmol_y_pred_new_9, convmol_y_pred_new_10,\n",
    "            convmol_y_pred_new_11, convmol_y_pred_new_12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06ffe69",
   "metadata": {},
   "source": [
    "## Balanced Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e85e2239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_accuracy(y_true, y_pred):\n",
    "    balanced_acc = []\n",
    "    for i in range(len(y_true)):\n",
    "        b_acc = balanced_accuracy_score(y_true[i], y_pred[i])\n",
    "        balanced_acc.append(b_acc)\n",
    "    return balanced_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "0a98f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles2img_balanced_acc = balanced_accuracy(smiles2img_y_true, smiles2img_y_pred_new)\n",
    "ecfp_balanced_acc = balanced_accuracy(ecfp_y_true, ecfp_y_pred_new)\n",
    "convmol_balanced_acc = balanced_accuracy(convmol_y_true, convmol_y_pred_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "16aa92fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.31336947626841244,\n",
       " 0.3699868073878628,\n",
       " 0.45777948398035734,\n",
       " 0.5042931571441045,\n",
       " 0.45710250201775626,\n",
       " 0.3756012506012506,\n",
       " 0.3673039923039923,\n",
       " 0.4813414019162885,\n",
       " 0.39967325737265413,\n",
       " 0.4577669761620631,\n",
       " 0.4952762579949141,\n",
       " 0.4006813156164711]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles2img_balanced_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3b6040cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3657223934634767,\n",
       " 0.32755417956656346,\n",
       " 0.3697851218899221,\n",
       " 0.4544877161580877,\n",
       " 0.40294117647058825,\n",
       " 0.4145915246832678,\n",
       " 0.462419470293486,\n",
       " 0.420445869598412,\n",
       " 0.42040915143445107,\n",
       " 0.467623199284044,\n",
       " 0.3960755813953488,\n",
       " 0.45919163545568037]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ecfp_balanced_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "1c90b959",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.28895738539067467,\n",
       " 0.3549707602339181,\n",
       " 0.38297939180698665,\n",
       " 0.37102687721931926,\n",
       " 0.3760504201680672,\n",
       " 0.35692442114460465,\n",
       " 0.4642090193271296,\n",
       " 0.46951188476612205,\n",
       " 0.36488722107896543,\n",
       " 0.3450301683073992,\n",
       " 0.3546511627906977,\n",
       " 0.43203807740324596]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convmol_balanced_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "0db98507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.42334798989717726, 0.41343725164111067, 0.38010306580309416]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "means= [np.mean(smiles2img_balanced_acc), np.mean(ecfp_balanced_acc), np.mean(convmol_balanced_acc)]\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "2836a13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4288919088171137, 0.41750033805885944, 0.36795704914914235]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medians= [np.median(smiles2img_balanced_acc), np.median(ecfp_balanced_acc), np.median(convmol_balanced_acc)]\n",
    "medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28598a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouped bar plot\n",
    "fig,ax = plt.subplots(figsize=(22,10))\n",
    "\n",
    "width = 0.2\n",
    "\n",
    "x=np.arange(12)\n",
    "plt.bar(x-0.2, smiles2img_balanced_acc, width, label='SmilesToImage', color='steelblue')\n",
    "plt.bar(x, ecfp_balanced_acc, width, label='ECFP', color='gold')\n",
    "plt.bar(x+0.2, convmol_balanced_acc, width, label='ConvMol', color='indianred')\n",
    "plt.title('Balanced Accuracy Score of Different Featurisers', fontsize=16)\n",
    "plt.ylabel('Balanced Accuracy Score', fontsize=14)\n",
    "plt.xlabel('Tox21 Assays', fontsize=14)\n",
    "\n",
    "#Bar labels\n",
    "plt.bar_label(ax.containers[0], fmt='%.3f', fontsize=12)\n",
    "plt.bar_label(ax.containers[1], fmt='%.3f', fontsize=12)\n",
    "plt.bar_label(ax.containers[2], fmt='%.3f', fontsize=12)\n",
    "\n",
    "#Assay labels\n",
    "plt.xticks(x, assays, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend()\n",
    "# plt.savefig('Overall BA.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouped bar plot - NR\n",
    "fig,ax = plt.subplots(figsize=(22,10))\n",
    "\n",
    "width = 0.2\n",
    "\n",
    "x=np.arange(7)\n",
    "\n",
    "plt.bar(x-0.2, smiles2img_balanced_acc[0:7], width, label='SmilesToImage', color='steelblue')\n",
    "plt.bar(x, ecfp_balanced_acc[0:7], width, label='ECFP', color='gold')\n",
    "plt.bar(x+0.2, convmol_balanced_acc[0:7], width, label='ConvMol', color='indianred')\n",
    "\n",
    "plt.title('Balanced Accuracy Score of Different Featurisers for Nuclear Receptor (NR) Panel', fontsize=16)\n",
    "plt.ylabel('Balanced Accuracy Score', fontsize=14)\n",
    "plt.xlabel('Nuclear Receptor Panel', fontsize=14)\n",
    "\n",
    "#Bar labels\n",
    "plt.bar_label(ax.containers[0], fmt='%.3f', fontsize=12)\n",
    "plt.bar_label(ax.containers[1], fmt='%.3f', fontsize=12)\n",
    "plt.bar_label(ax.containers[2], fmt='%.3f', fontsize=12)\n",
    "\n",
    "#Assay labels\n",
    "plt.xticks(x, assays[0:7],fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend()\n",
    "# plt.savefig('Overall BA-NR.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18c0333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouped bar plot - SR\n",
    "fig,ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "width = 0.2\n",
    "\n",
    "x=np.arange(5)\n",
    "\n",
    "plt.bar(x-0.2, smiles2img_balanced_acc[7:12], width, label='SmilesToImage', color='steelblue')\n",
    "plt.bar(x, ecfp_balanced_acc[7:12], width, label='ECFP', color='gold')\n",
    "plt.bar(x+0.2, convmol_balanced_acc[7:12], width, label='ConvMol', color='indianred')\n",
    "\n",
    "plt.title('Balanced Accuracy Score of Different Featurisers for Stress Response (SR) Panel', fontsize=16)\n",
    "plt.ylabel('Balanced Accuracy Score', fontsize=14)\n",
    "plt.xlabel('Stress Response Panel', fontsize=14)\n",
    "\n",
    "#Bar labels\n",
    "plt.bar_label(ax.containers[0], fmt='%.3f', fontsize=12)\n",
    "plt.bar_label(ax.containers[1], fmt='%.3f', fontsize=12)\n",
    "plt.bar_label(ax.containers[2], fmt='%.3f', fontsize=12)\n",
    "\n",
    "#Assay labels\n",
    "plt.xticks(x, assays[7:12],fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend()\n",
    "# plt.savefig('Overall BA-SR.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "27e6c25a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Area Under the Receiver Operating Characteristic(ROC) Curve\n",
    "def roc_auc(y_true, y_pred):\n",
    "    rocauc=[]\n",
    "    for i in range(len(y_true)):\n",
    "        auroc = roc_auc_score(y_true[i], y_pred[i])\n",
    "        rocauc.append(auroc)\n",
    "    return rocauc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9a4f405f",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles2img_auc = roc_auc(smiles2img_y_true, smiles2img_y_pred_new)\n",
    "ecfp_auc = roc_auc(ecfp_y_true, ecfp_y_pred_new)\n",
    "convmol_auc = roc_auc(convmol_y_true, convmol_y_pred_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8f2c0c28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3432896890343699,\n",
       " 0.42519788918205803,\n",
       " 0.43983236559655775,\n",
       " 0.4902348866315452,\n",
       " 0.4271388216303471,\n",
       " 0.3731361231361231,\n",
       " 0.4781144781144781,\n",
       " 0.5038144743137178,\n",
       " 0.4522871983914209,\n",
       " 0.4779375382018221,\n",
       " 0.46836711104261386,\n",
       " 0.45844373169711333]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles2img_auc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41930823",
   "metadata": {},
   "source": [
    "From the evaluation of the three models, the CNN model with the SmilesToImage featuriser was observed to have the highest average balanced accuracy score. This model was used to build an autoencoder to improve model performance. To cross check the performance of the models, the roc-auc score was used. However, as observed in previous the featuriser notebooks, this was calculated to be identical values as the balanced accuracy scores, so no graphs were plotted for this metric. \n",
    "\n",
    "Additionally, to investigate the effect of epochs on average balanced accuracy score, the CNN model was run with different epochs, ranging from 1-15. The results were compiled in a separate Excel spreadsheet for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ffd05c",
   "metadata": {},
   "source": [
    "## Building an autoencoder using the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69e9e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting SmilesToImage featurised dataset into smaller train,valid,test datasets\n",
    "splitter = dc.splits.RandomSplitter()\n",
    "\n",
    "train_smiles_small, valid_smiles_small,test_smiles_small = splitter.train_valid_test_split(\n",
    "    datasets_smiles[0], \n",
    "    frac_train = 0.8, frac_valid = 0.08, frac_test = 0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b243358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Filtering out empty arrays in featurised datasets\n",
    "# train_feat = []\n",
    "# for i in range(len(train_smiles_small.X)):\n",
    "#     if train_smiles_small.X[i].shape != (0,):\n",
    "#         train_feat.append(train_smiles_small.X[i])\n",
    "        \n",
    "# valid_feat = []\n",
    "# for i in range(len(valid_smiles_small.X)):\n",
    "#     if valid_smiles_small.X[i].shape != (0,):\n",
    "#         valid_feat.append(valid_smiles_small.X[i])\n",
    "\n",
    "# test_feat = []\n",
    "# for i in range(len(test_smiles_small.X)):\n",
    "#     if test_smiles_small.X[i].shape != (0,):\n",
    "#         test_feat.append(test_smiles_small.X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Converting numpy array to tensor\n",
    "# train_tensor = []\n",
    "# for i in range(len(train_feat)):\n",
    "#     tensor = tf.convert_to_tensor(train_feat[i])\n",
    "#     train_tensor.append(tensor)\n",
    "    \n",
    "# valid_tensor = []\n",
    "# for i in range(len(valid_feat)):\n",
    "#     tensor_v = tf.convert_to_tensor(valid_feat[i])\n",
    "#     valid_tensor.append(tensor_v)\n",
    "    \n",
    "# test_tensor = []\n",
    "# for i in range(len(test_feat)):\n",
    "#     tensor_t = tf.convert_to_tensor(test_feat[i])\n",
    "#     test_tensor.append(tensor_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8618290d",
   "metadata": {},
   "source": [
    "To start building the autoencoder, the keras blog was used for reference of building a 2D convolutional autoencoder. Firstly, a simple convolutional autoencoder of one convolutional layer was built. The autoencoder was improved by adding more layers such as the max pooling layer and flatten layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9dbe2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Simple convolutional autoencoder\n",
    "\n",
    "# # This is the size of our encoded representations\n",
    "# encoding_dim = 32  \n",
    "\n",
    "# input_img = layers.Input(shape=(40,40,1))\n",
    "\n",
    "# #2D-Convolutional layer\n",
    "# Conv2d_1 = layers.Conv2D(filters=32,\n",
    "#                           kernel_size=3,\n",
    "#                           activation='relu',\n",
    "#                           dilation_rate=2)(input_img)\n",
    "\n",
    "# # \"encoded\" is the encoded representation of the input\n",
    "# encoded = layers.Dense(encoding_dim, activation='relu')(Conv2d_1)\n",
    "\n",
    "# # \"decoded\" is the loss reconstruction of the input\n",
    "# decoded = layers.Dense(784, activation='relu')(encoded)\n",
    "\n",
    "# # This model maps an input to its reconstruction\n",
    "# autoencoder = keras.Model(input_img, decoded)\n",
    "\n",
    "# # This model maps an input to its encoded representation\n",
    "# encoder = keras.Model(input_img, encoded)\n",
    "\n",
    "# # This is our encoded (32-dimensional) input\n",
    "# encoded_input = keras.Input(shape=(encoding_dim,))\n",
    "# # Retrieve the last layer of the autoencoder model\n",
    "# decoder_layer = autoencoder.layers[-1]\n",
    "\n",
    "# # Create the decoder model\n",
    "# decoder = keras.Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "62abe954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Adding more layers to the convolutional autoencoder\n",
    "\n",
    "# # This is the size of our encoded representations\n",
    "# encoding_dim = 32  \n",
    "\n",
    "# input_img = layers.Input(shape=(40,40,1))\n",
    "\n",
    "# #2D-Convolutional layer 1\n",
    "# Conv2D_1 = layers.Conv2D(filters=32,\n",
    "#                           kernel_size=3,\n",
    "#                           activation='relu',\n",
    "#                           dilation_rate=2)(input_img)\n",
    "\n",
    "# #Max pooling layer 1\n",
    "# MaxPool2D_1 = layers.MaxPooling2D((2, 2), padding='same')(Conv2D_1)\n",
    "\n",
    "# #2D-Convolutional layer 2\n",
    "# Conv2D_2 = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(MaxPool2D_1)\n",
    "\n",
    "# #Max pooling layer 2\n",
    "# MaxPool2D_2 = layers.MaxPooling2D((2, 2), padding='same')(Conv2D_2)\n",
    "\n",
    "# #Flatten layer\n",
    "# Flatten = layers.Flatten()(MaxPool2D_2)\n",
    "\n",
    "# encoded = layers.Dense(encoding_dim, activation='relu')(Flatten)\n",
    "\n",
    "\n",
    "# # \"decoded\" is the loss reconstruction of the input\n",
    "# decoded = layers.Dense(784, activation='relu')(encoded)\n",
    "\n",
    "# # This model maps an input to its reconstruction\n",
    "# autoencoder = keras.Model(input_img, decoded)\n",
    "\n",
    "# # This model maps an input to its encoded representation\n",
    "# encoder = keras.Model(input_img, encoded)\n",
    "\n",
    "# # This is our encoded (32-dimensional) input\n",
    "# encoded_input = keras.Input(shape=(encoding_dim,))\n",
    "\n",
    "# # Retrieve the last layer of the autoencoder model\n",
    "# decoder_layer = autoencoder.layers[-1]\n",
    "\n",
    "# # Create the decoder model\n",
    "# decoder = keras.Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78f7730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Compiling autoencoder\n",
    "# autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9b8df333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Training the autoencoder\n",
    "# autoencoder_test = autoencoder.fit(x=train_tensor,\n",
    "#                                    y=train_tensor,\n",
    "#                                    epochs=10,\n",
    "#                                    batch_size=512,\n",
    "#                                    shuffle=True,\n",
    "#                                   validation_data=(valid_tensor, valid_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dbb8af",
   "metadata": {},
   "source": [
    "The above autoencoder model was not able to train on the training data so another method of implementing the keras model was used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b9c6c358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Constructing the model\n",
    "\n",
    "# keras_model = keras.Sequential()\n",
    "# keras_model.add(layers.Conv2D(filters=32,\n",
    "#                           kernel_size=3,\n",
    "#                           activation='relu',\n",
    "#                           dilation_rate=2,\n",
    "#                             input_shape=(80, 80, 1)))\n",
    "# keras_model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "# keras_model.add(layers.Dense(50, activation='relu'))\n",
    "# keras_model.add(layers.Dense(1))\n",
    "\n",
    "# keras_model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "396b0600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f245c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_model.fit(x=train_tensor,\n",
    "#                 y=train_tensor,\n",
    "#                 epochs=10,\n",
    "#                 batch_size=512,\n",
    "#                 shuffle=True,\n",
    "#                validation_data=(valid_tensor, valid_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daba284d",
   "metadata": {},
   "source": [
    "However, this autoencoder also does not work. This could be due to the Tox21 dataset containing multi-dimensional data. To improve on the training of the model, the Tox21 dataset could be split into each of the 12 assays and train the model with each individual assay to start with."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
