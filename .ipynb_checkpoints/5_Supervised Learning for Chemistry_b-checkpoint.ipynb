{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning Using DeepChem\n",
    "\n",
    "This is the last notebook to do as part of this course (reinforcement learning, which is the missing third of ML is currently used less for ML, but keep an eye out for it being used more in retrosynthesis in the future). \n",
    "\n",
    "At this point, I thought I would give you some resources to continue learning from: \n",
    "\n",
    "*Paid*\n",
    "\n",
    "1. O'Reilly, 'Hands on machine learning with scikit-learn and tensorflow (2nd edition), Aurelien Geron\n",
    "https://www.oreilly.com/library/view/hands-on-machine-learning/9781492032632/\n",
    "If you were to buy one ML textbook I would get this one. It's about Â£40. Make sure you get the 2nd edition as that includes tensorflow2 which is the newer (and ever so much easier to use) version of tensorflow. The code used in the textbook is here: https://github.com/ageron/handson-ml2\n",
    "\n",
    "2. O-Reilly, /'Deep learning for the life sciences'/\n",
    "https://www.oreilly.com/library/view/deep-learning-for/9781492039822/\n",
    "This is OK, it has some nice deepchem examples, however, the book is rather short and half of it is biology related, so this is best a book that is borrowed from the library\n",
    "\n",
    "*Free*\n",
    "\n",
    "1. DeepChem's tutorials: \n",
    "https://github.com/deepchem/deepchem/tree/master/examples\n",
    "Download or pull these and run them in the environment set up for these notebooks. They will introduce you to deepchem and covers more material than this course was able to \n",
    "\n",
    "2. Deep Learning for hte life sciences example code\n",
    "https://github.com/deepchem/DeepLearningLifeSciences\n",
    "The code from the book above. Useful to look at if you're not able to borrow the book. \n",
    "\n",
    "You do not need to buy any books if you don't want to!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DeepChem\n",
    "\n",
    "You will be using several different modules here\n",
    "\n",
    "**DeepChem** [1,2] is a lovely module that has only recently appeared, and it will make using ML for chemistry so much easier! In this section we shall play with it and try to solve some chemical problems.\n",
    "\n",
    "\n",
    "**MoleculeNet** [3,4] is a series of _benchmark_ datasets for chemistry, these are standard datasets that people can use to test their algorithms against. Also, they are problems worth solving in their own right. \n",
    "\n",
    "**Tensorflow** [5] is a ML package that we met in the previous workbook, it is powerful and useful, however writing tensorflow code directly is fiddly (as least I think so!)\n",
    "\n",
    "**Keras** [6] is a module for writing, training and testing ML/neural networks (NN), it will write the correct tensorflow code for you. \n",
    "\n",
    "DeepChem contains both keras and tensorflow, so you can use those modules from within it. \n",
    "\n",
    "rdkit is a general purpose computational chemistry module which we will use for plotting molecules, but you can also use it to calculate molecular properties.\n",
    "\n",
    "[1] https://deepchem.io/\n",
    "\n",
    "[2] https://github.com/deepchem\n",
    "\n",
    "[3] Wu, Zhenqin, et al. \"MoleculeNet: a benchmark for molecular machine learning.\" Chemical science 9.2 (2018): 513-530.\n",
    "\n",
    "[4] https://deepchem.readthedocs.io/en/latest/moleculenet.html\n",
    "\n",
    "[5] https://keras.io/\n",
    "[6] https://www.tensorflow.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (current_repodata.json): ...working... done\n",
      "Solving environment: ...working... done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "Retrieving notices: ...working... done\n"
     ]
    }
   ],
   "source": [
    "!conda install -c conda-forge scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rdkit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_15024\\1031187960.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# lets load our libraries\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdeepchem\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mrdkit\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mChem\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mrdkit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mChem\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDraw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'rdkit'"
     ]
    }
   ],
   "source": [
    "# lets load our libraries\n",
    "import deepchem as dc\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Supervised Learning\n",
    "\n",
    "Supervised learning is when the algorithm is given the answer (target) to learn from. Essentially, the algorithm is given in the input data, it guesses a solution to get a prediction (output), and this is compared to the correct answer. Obviously the first guess is wrong, but by comparing the prediction ($\\hat{\\textbf{y}}$) to the true answer, *target* or *ground truth* (**y**) the model can *learn* and improve its predictions.\n",
    "\n",
    "Supervised learning models *fit* a *function* from *features* (**X**) to *target(s)* (**y**). For example, the features may be a set of *fingerprints* (created from a chemical structure) and the target may be a molecular property, like atomic energy, solubility, or action against a protein target.\n",
    "\n",
    "There are many supervised learning algorithms, but most the recent breakthroughs in ML have come from **neural networks** and this is what we will concentrate on here. This notebook contains chemical models, see 4_Supervised_Learning for neural networks applied to standard NN problems (vision) to help you understand how NNs work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Types of problems: \n",
    "\n",
    "#### Regression\n",
    "\n",
    "The regression discussed here is exactly the same as that in notebooks 1 and 2a, however as the input data is highly multidimentional (often 1000's of dimensions), the fitted function is a multi-dimensional object, for this reason we do not plot the fitted function (as we did in the Linear_Regression_Cellulose notebook). Instead, we are interested in how well the model *predicts* the training set data, and *generalises* to the test data. \n",
    "\n",
    "Regression models can be thought of as fitting a function:\n",
    "$$\n",
    "\\textbf{X} \\rightarrow Model \\rightarrow \\textbf{y}\n",
    "$$\n",
    "\n",
    "where **y** is usually a number. \n",
    "\n",
    "#### Classificiation\n",
    "\n",
    "Classification models assign input data to a *category*. For example, matching a molecular structure to the category 'active against HIV protein'. See notebook 4 for more examples. \n",
    "\n",
    "#### Generation\n",
    "\n",
    "Generative models produce new examples of the input after training. For example, you might train a variational autoencoder on molecular structures and then use it to generate novel molecular structures. \n",
    "\n",
    "In this document we shall train some regression and classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "When training an ML algorithm, it is common practice to split your data into 2 or 3 datasets:\n",
    "\n",
    "1. Train: this is the majority of the data, and it is used to train the ML algorithm. \n",
    "2. Test: this is data that is NEVER shown to the algorithm during training and only used at the end to test the algorithm.\n",
    "3. Validation: this is data that is not used for training but is used to test the algorithm during training (validate the algorithm). \n",
    "\n",
    "The point of building supervised ML algorithms is not generally to describe your data, but to use the model in the future to predict **y** for new data. For example, in the IMPRESSION NMR prediction program [[]] input molecular structure is mapped to output chemical shifts. This is not used to model what the NMR is for the molecules in the dataset (as we already know that, we used it to train the algorithm in the first place!), but to predict the chemical shifts for an unseen molecule, for example, one that a synthetic chemist has just made.\n",
    "\n",
    "The test set is held out and not used for training to get an idea how an algorithm will work on as-yet-unseens data. N.B. This is the equivalent of the leave-one-out approach used in design of experiments and regression models, only here instead of leaving a single datapoint out of a small dataset, we leave a fraction (usually 10%) out. \n",
    "\n",
    "As ML algorithms have such a large degree of power to fit data, they have the problem of *over-fitting*, where the algorithm learns to memorise the dataset, and this makes the algorithm less useful for generalisation. So, we use the validation dataset as a practise test set. The algorithm is tested on this dataset after every epoch of training, and overfitting happens once the validation set stops improving. \n",
    "\n",
    "Common splits for the data are:\n",
    "Train: 90% : Test: 10% \n",
    "or Train: 80%, Validation: 10%, Test: 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delaney Dataset for drug solubility\n",
    "\n",
    "The Delaney (ESOL) dataset [7] a regression dataset containing structures and water solubility data for 1128 compounds. The dataset is widely used to validate machine learning models on estimating solubility directly from molecular structures (as encoded in SMILES strings). Solubility is vital to achieve desired concentration of drug for anticipated pharmacological response. \n",
    "\n",
    "[7] Delaney, J. S. *Journal of Chemical Information and Modeling*, **44**, 1000â1005 (2004)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepChem has methods to load datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line loads the Delaney dataset\n",
    "tasks, datasets, transformers = dc.molnet.load_delaney(featurizer='ECFP')\n",
    "# the datasets object is already split into the train, validation and test dataset \n",
    "train_dataset, valid_dataset, test_dataset = datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some example data from the test set. There are 1049 features generated from the SMILES strings (scroll right to see the SMILES). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Featurizing inputs\n",
    "\n",
    "To build a NN that can categorize images (i.e. map a picture of a cat to 'cat') you use images at inputs.\n",
    "\n",
    "It is not immediately obvious to know how best to input molecular structure. SMILES strings are designed to computer readable formulae, these can be convereted to test inputs (strings). There are several ways to extract the information from molecular structure, and this is called featurizing the molecule. \n",
    "\n",
    "Some examples:\n",
    "\n",
    "*SMILES strings*: 'Cc1cc(=O)[nH]c(=S)[nH]1'\n",
    "\n",
    "*Molecular descriptors*: a list of physicochemical properties about the molecule, for example: molecular weight, boiling point, dipole moment etc (these were used in notebook 2 and called `solvent parameters`).\n",
    "\n",
    "*Molecular graphs*: the chemical formula (as you would draw by hand)\n",
    "\n",
    "*Extended Connectiviy FingerPrints (ECFP)*: this is a vector of 1s and 0s that indicates the presence of absence of features like atoms, functional groups, number of covalent bonds etc. This is the featurization that we're using here.\n",
    "\n",
    "*voxels*: a voxel is a 3-D pixel, i.e. a cube of space. This featurisation encodes 3D structural data by indicating hte presence or absence of an atom in that cube of space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " The code below tells you about the dataset. \n",
    " `X` (the input) has 113 rows (corresponding to 113 molecules) and 1024 columns corresponding to 1024 features, with `ids` (identification) which are SMILEs strings,\n",
    " and `task_names` which is the name of the target, in this case, the `measured log solubility in mols per litre` for the molecule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at the molecules\n",
    "\n",
    "To display a molecular graph (or chemical formula, as you know it) we use `rdkit`'s `Chem` module imported above to read SMILEs strings: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look, a molecule!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "molecule='Oc1ccc(cc1)C2(OC(=O)c3ccccc23)c4ccc(O)cc4 '\n",
    "Chem.MolFromSmiles(molecule)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *decision tree* is a simple ML algorithm which is basically a flow chart of yes/no questions. For example:\n",
    "\n",
    "\n",
    "`-OH present?\n",
    "    yes -->\n",
    "        C=0 present?\n",
    "            yes --> carboxylic acid\n",
    "            no --> alcohol\n",
    "    no --> etc `\n",
    "\n",
    "A *random forest* is a collection (a forest in fact) of decision trees. see:\n",
    "https://towardsdatascience.com/understanding-random-forest-58381e0602d2\n",
    "\n",
    "Random forests are simple ML algorithms, and as such they are easy to tune and are a good algorithm to start with. They are often then used as a benchmark (to beat) for more complicated algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this loads in the method used to create the RandomForestRegerssor model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# This line creates a new Random forest regressor object, which we give the name: RF_model\n",
    "# this is an Sklearn model\n",
    "RF_model = RandomForestRegressor(n_estimators=100)\n",
    "# This line tells deepchem that RF_model is a SKleanr model and gives it a new name: model\n",
    "model = dc.models.SklearnModel(RF_model)\n",
    "############################################\n",
    "# Now we fit the training dataset!             #\n",
    "############################################\n",
    "model.fit(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find out how well our model performs, we use what is called a *metric*. The word metric basically means a ruler, and we use it to measure the model. \n",
    "\n",
    "For regression models, the most commonly used metric is the R$^2$ score (technically called the Pearson R$^2$ score). This is the R$^2$ we met in the linear regression workbook. \n",
    "\n",
    "A high R$^2$ on the training dataset tells us that the model is learning well. \n",
    "A high R$^2$ on the test dataset tells us that we have a good model for generalisation, i.e. one that is useful!\n",
    "\n",
    "Note that the `transformer` is the method we used to put the data in the correct format (we got that from the dataset above)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line tells deepchem what metric to use to score the datasets\n",
    "metric = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
    "\n",
    "# model.evaluate() tests the model. \n",
    "# we have to give it the data to use, the metric (or set of metrics) and the transformer used\n",
    "\n",
    "print(\"Training set score:\", model.evaluate(train_dataset, [metric], transformers))\n",
    "print(\"Test set score:\", model.evaluate(test_dataset, [metric], transformers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the random forest trains well on the data, but does not make a good predictive model.\n",
    "\n",
    "Let's see what those predictions are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line picks a batch of 12 molecules and gets the predicted solubilities\n",
    "solubilities = model.predict_on_batch(test_dataset.X[:12])\n",
    "# this grabs the first 12 smiles strings as well\n",
    "SMILES_strings = test_dataset.ids[:12]\n",
    "# this line picks a batch of 12 molecules and gets the predicted solubilities\n",
    "ground_truth = test_dataset.y[:12]\n",
    "# this makes an empty list called mol_list\n",
    "mol_list=[]\n",
    "print('\\tSolubility\\t\\tMolecule')\n",
    "print('Predicted\\tActual\\t\\t\\n')\n",
    "# this loops over the numbers 0 to 11 ...\n",
    "for i in range(12):\n",
    "    print(\"{:.3}\\t\\t{:.3}\\t\\t{}\".format(solubilities[i], ground_truth[i][0], SMILES_strings[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and this draws a nice image of the 12 molecules\n",
    "ms = [Chem.MolFromSmiles(x) for x in SMILES_strings]\n",
    "Draw.MolsToGridImage(ms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph convolution networks\n",
    "\n",
    "So the random forest wasn't that great a predictive model, can we do better?\n",
    "\n",
    "Neural networks are very powerful supervised learning algorithms, and we're going to use one specialised for molecular graphs (like those shown above). \n",
    "\n",
    "to do this we reload the Delaney dataset, this time using the graph convolution featurizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks, datasets, transformers = dc.molnet.load_delaney(featurizer='GraphConv')\n",
    "train_dataset, valid_dataset, test_dataset = datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the code to make a graph convolution model. \n",
    "\n",
    "`Dropout` is a technique to avoide over-fitting, it randomly disconnects some of the neurons in the neural network forcing the algorithm to involve many neurons in the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we build a graph convolutional model and call it model\n",
    "model = dc.models.GraphConvModel(n_tasks=1, mode='regression', dropout=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train it\n",
    "model.fit(train_dataset, nb_epoch=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
    "print(\"Training set score:\", model.evaluate(train_dataset, [metric], transformers))\n",
    "print(\"Test set score:\", model.evaluate(test_dataset, [metric], transformers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here we can look at the results for that batch of 12 examples from the test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line picks a batch of 12 molecules and gets the predicted solubilities\n",
    "solubilities = model.predict_on_batch(test_dataset.X[:12])\n",
    "# this grabs the first 12 smiles strings as well\n",
    "SMILES_strings = test_dataset.ids[:12]\n",
    "# this line picks a batch of 12 molecules and gets the predicted solubilities\n",
    "ground_truth = test_dataset.y[:12]\n",
    "# this makes an empty list called mol_list\n",
    "mol_list=[]\n",
    "print('\\tSolubility\\t\\tMolecule')\n",
    "print('Predicted\\tActual\\t\\t\\n')\n",
    "# this loops over the numbers 0 to 11 ...\n",
    "for i in range(12):\n",
    "    print(\"{:.3}\\t\\t{:.3}\\t\\t{}\".format(solubilities[i][0], ground_truth[i][0], SMILES_strings[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the neural network model has worked much better at predicting solubility on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making your own models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will write our own keras code to create different neural networks and see which is best for solving the delaney dataset.\n",
    "\n",
    "Have a look at the neural networks powerpoint to find out what these things are. We will make a simple *fully-connected* neural network which takes in the ECFP fingerprints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks, datasets, transformers = dc.molnet.load_delaney(featurizer='ECFP', splitter='random')\n",
    "train_dataset, valid_dataset, test_dataset = datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the neural network. \n",
    "\n",
    "`tf.keras.Sequential` is a *wrapper* function that tells keras that we're using a sequential model, i.e. a simple neural network. \n",
    "\n",
    "The data in input into the network and then flows into:\n",
    "`tf.keras.layers.Dense(50, activation='sigmoid')`\n",
    "this is a standard *fully connected* layer (which keras calls `Dense` as there are many many connections). The first number, `50` tells keras how many units we want in the layer. The `activation function` is a mathematical function whose purpose is to add *non-linearity* to the system. Neural networks can be described as a type of *non-linear regression model*, they are very powerful and part of that power comes from the non-linearity. \n",
    "\n",
    "The data flows through the first layer (the one with 50 units) and into the second (and final) layer:\n",
    "`tf.keras.layers.Dense(1)`\n",
    "this has only 1 unit and no non-linearity (the default activation is linear). For regression models we read out the number that flows out of this single unit, and that is our prediction!\n",
    "\n",
    "This is a *tiny* neural network. It has an input layer (not shown as keras assumes you have an input!), a single *hidden layer* (this one: `tf.keras.layers.Dense(50, activation='sigmoid')`) and an *output layer* (this one: `tf.keras.layers.Dense(1)`), so this is a single hidden layer model, a *shallow* network. These were the types of NNs that people used in the 1990s, however, it was then discovered that adding more hidden layers made more powerful NNs. A network with more than one hidden layer is called a *deep neural network*. Now, there are deep NNs with hundreds of layers, usually of many different types. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(50, activation='sigmoid'), # hidden layer\n",
    "    tf.keras.layers.Dense(1) # output layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, in deepchem we must wrap our keras_model in a wrapper function `dc.models.KerasModel()` to tell deepchem what it is. The second term in this wrapper function is the `loss function`, this is the function that tells deepchem what to `minimize` to train the model. To train the model, we want to minimize the error between the predicted output $\\hat{\\mathbf{y}}$ and the ground truth $\\mathbf{y}$. There is more than one solution that the NN could find, so we use the loss function to guide it towards a solution we think would be good, we do this by adding in an extra error term which it also has to minimize and which is higher the further the NN is from the type of solution we think would be good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# makes a deep chem model from our keras model, and tells it what loss function to use \n",
    "model = dc.models.KerasModel(keras_model, dc.models.losses.L1Loss())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Neural networks require seeing the data more than once to learn. Each time we show it a copy of the training data is an *epoch*. Here we train for 50 epochs, which is 50 presentations of the data. Usually, you set up the training to stop automatically once you reach a good solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fits the model\n",
    "model.fit(train_dataset, nb_epoch=50)\n",
    "# sets R2 as the metric\n",
    "metric = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
    "# tests and prints out the model\n",
    "print('training set score:', model.evaluate(train_dataset, [metric]))\n",
    "print('test set score:', model.evaluate(test_dataset, [metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Making a good model to predict drug solubility from structure\n",
    "\n",
    "Can you improve on the model above? \n",
    "\n",
    "The best I've found is an $R^2$ *on the test dataset*  of 0.74. Can you beat it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Things to add:\n",
    "\n",
    "*types of layers*\n",
    "1. tf.keras.layers.Dense(1000, activation='relu')\n",
    "2. tf.keras.layers.Dropout(rate=0.5)\n",
    "3. tf.keras.layers.BatchNormalization(),\n",
    "\n",
    "You can change the number of neurons: \n",
    "\n",
    "`tf.keras.layers.Dense(*1000*, activation='relu')` --> `tf.keras.layers.Dense(*200*, activation='relu')`\n",
    "\n",
    "You can change the activation function:\n",
    "\n",
    "`tf.keras.layers.Dense(1000, activation=*'relu'*)` --> `tf.keras.layers.Dense(1000, activation=*'sigmoid'*)`\n",
    "\n",
    "You can add in normalisation or dropout layers:\n",
    "\n",
    "`tf.keras.layers.Dropout(rate=0.5) (and you can change the rate)`\n",
    "`tf.keras.layers.BatchNormalization()`\n",
    "\n",
    "These are applied to `Dense` layers so need to come after them, e.g. this model still has only 1 hidden layer, but that layer has dropout applied to it: \n",
    "\n",
    "`keras_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(50, activation='sigmoid'),\n",
    "    tf.keras.layers.Dropout(rate=0.2),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])`\n",
    "\n",
    "You can change the loss function, here's some to try:\n",
    "\n",
    "`dc.models.losses.L1Loss()`\n",
    "`dc.models.losses.L2Loss()`\n",
    "`dc.models.losses.L1Loss()`\n",
    "`dc.models.losses.SigmoidCrossEntropy()`\n",
    "`dc.models.losses.BinaryCrossEntropy()`\n",
    "\n",
    "Note that each time you train a NN, you get a different solution, so the same model may give better or worse solutions to the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(50, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model = dc.models.KerasModel(keras_model, dc.models.losses.L1Loss())\n",
    "\n",
    "model.fit(train_dataset, nb_epoch=50)\n",
    "metric = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
    "print('training set score:', model.evaluate(train_dataset, [metric]))\n",
    "print('test set score:', model.evaluate(test_dataset, [metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1000, activation='relu'),\n",
    "    #tf.keras.layers.Dropout(rate=0.5),\n",
    "    tf.keras.layers.Dense(500, activation='relu'),\n",
    "    #tf.keras.layers.Dropout(rate=0.5),\n",
    "    tf.keras.layers.Dense(500, activation='relu'),\n",
    "    #tf.keras.layers.Dropout(rate=0.5),\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    #tf.keras.layers.Dropout(rate=0.5),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model = dc.models.KerasModel(keras_model, dc.models.losses.L2Loss())\n",
    "\n",
    "model.fit(train_dataset, nb_epoch=50)\n",
    "metric = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
    "print('training set score:', model.evaluate(train_dataset, [metric]))\n",
    "print('test set score:', model.evaluate(test_dataset, [metric]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1000, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(500, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.5),\n",
    "    tf.keras.layers.Dense(100, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "model = dc.models.KerasModel(keras_model, dc.models.losses.L2Loss())\n",
    "\n",
    "model.fit(train_dataset, nb_epoch=50)\n",
    "metric = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
    "print('training set score:', model.evaluate(train_dataset, [metric]))\n",
    "print('test set score:', model.evaluate(test_dataset, [metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificiation\n",
    "\n",
    "In this section, we will classify the input molecules. Here, we are looking for which molecules are active at prevented HIV replication. (Notebook 4 gives further detail on what classification is). \n",
    "\n",
    "#### The HIV dataset\n",
    "\n",
    "\"The HIV dataset was introduced by the Drug Therapeutics Program (DTP) AIDS AntiviralScreen,  which  tested  the  ability  to  inhibit  HIV  replication  for  over  40,000  compounds. 47 Screening results were evaluated and placed into three categories:  confirmed inactive (CI), confirmed active (CA) and confirmed moderately active (CM). We further combine the latter  two  labels,  making  it  a  classification  task  between  inactive  (CI)  and  active  (CA  and CM). As we are more interested in discover new categories of HIV inhibitors, scaffold split-ting(introduced in the next subsection) is recommended for this dataset.\" (Taken from the MoleculeNet paper). \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before, we download and featurize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks, datasets, transformers = dc.molnet.load_hiv(feturizer='ECFP', splitter='scaffold')\n",
    "train_dataset, valid_dataset, test_dataset = datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now make a classification model. \n",
    "\n",
    "This has 1 output, which will be a binary number (1 or 0) indicating whether the drug molecule is active against HIV (1) or not active (0). This is a very hard problem and one under active research. We're asking the neural network to learn what molecules are, and approximate, from the data, which aspects of the molecule might disrupt a complex process involving a virus and cells. There are many different places in this process where the drug could be acting. Frankly, I'm surprised that this works at all! The fact that it does work is a testiment to how powerful neural networks are. \n",
    "\n",
    "We use a different metric here. For the regression model we used $R^2$, in notebook 4, we used *accuracy*, here we use *receiver-operator characteristic* **ROC**. (The term comes from sending signals over the radio), and is a different measure of the accuracy of the model. It compares the predictions it got correct: true positives (those it said were active and are) and true negatives (those it said were inactive which are inactive) with the predictions it got incorrect: false positives (molecules it said were active, but which are not) and false negatives (those it said were inactive, but which work). \n",
    "\n",
    "And ROC of 1 is perfect, and ROC of 0.5 is the same as guessing randomly (i.e. not very good at all!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(500, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='relu')\n",
    "])\n",
    "model = dc.models.KerasModel(classification_model, dc.models.losses.SigmoidCrossEntropy())\n",
    "\n",
    "model.fit(train_dataset, nb_epoch=100)\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "print('training set score:', model.evaluate(train_dataset, [metric]))\n",
    "print('test set score:', model.evaluate(test_dataset, [metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Make a better classification neural network to screen anti-viral drugs for action against HIV\n",
    "\n",
    "Try modifying the NN below to see if you can improve the ROC score on the test set. \n",
    "\n",
    "the best I've managed is 0.70"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(500, activation='relu'),\n",
    "        tf.keras.layers.Dense(1, activation='relu')\n",
    "])\n",
    "model = dc.models.KerasModel(classification_model, dc.models.losses.SigmoidCrossEntropy())\n",
    "\n",
    "model.fit(train_dataset, nb_epoch=100)\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "print('training set score:', model.evaluate(train_dataset, [metric]))\n",
    "print('test set score:', model.evaluate(test_dataset, [metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1000, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.5),\n",
    "    tf.keras.layers.Dense(500, activation='relu'),\n",
    "    tf.keras.layers.Dropout(rate=0.5),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    #tf.keras.layers.Dense(1,activation='softmax'),\n",
    "])\n",
    "model = dc.models.KerasModel(classification_model, dc.models.losses.BinaryCrossEntropy())\n",
    "\n",
    "model.fit(train_dataset, nb_epoch=100)\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "print('training set score:', model.evaluate(train_dataset, [metric]))\n",
    "print('test set score:', model.evaluate(test_dataset, [metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1000, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(500, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    #tf.keras.layers.Dense(1,activation='softmax'),\n",
    "])\n",
    "model = dc.models.KerasModel(classification_model, dc.models.losses.BinaryCrossEntropy())\n",
    "\n",
    "model.fit(train_dataset, nb_epoch=100)\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "print('training set score:', model.evaluate(train_dataset, [metric]))\n",
    "print('test set score:', model.evaluate(test_dataset, [metric]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(1000, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1000, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(200, activation='relu'),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    #tf.keras.layers.Dense(1,activation='softmax'),\n",
    "])\n",
    "model = dc.models.KerasModel(classification_model, dc.models.losses.BinaryCrossEntropy())\n",
    "\n",
    "model.fit(train_dataset, nb_epoch=100)\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "print('training set score:', model.evaluate(train_dataset, [metric]))\n",
    "print('test set score:', model.evaluate(test_dataset, [metric]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOP HERE:\n",
    "\n",
    "this is the end of this notebook. \n",
    "\n",
    "Go and do notebook 4 if you haven't!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda install -c conda-forge -y pdbfixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks, datasets, transformers = dc.molnet.load_pdbbind(featurizer='grid', split='random')\n",
    "train_dataset, valid_dataset, test_dataset = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features=train_dataset.X.shape[1]\n",
    "model = dc.models.MultitaskRegressor(\n",
    "    n_tasks=len(tasks),\n",
    "    n_features=n_features,\n",
    "    layer_sizes=[2000,1000],\n",
    "    dropouts=0.5,\n",
    "    learning_rate=0.0003)\n",
    "model.fit(train_dataset, nb_epoch=250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting datasets to ensure a fair test\n",
    "\n",
    "splitters = ['random', 'scaffold', 'butina']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_method = 'butina'\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "\n",
    "tasks, datasets, transformers = dc.molnet.load_tox21(featurizer='ECFP', split=split_method)\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "model = dc.models.MultitaskClassifier(n_tasks=len(tasks), n_features=1024, layer_sizes=[1000])\n",
    "model.fit(train_dataset, nb_epoch=10)\n",
    "print('splitter:', splitter)\n",
    "print('training set score:', model.evaluate(train_dataset, [metric], transformers))\n",
    "print('test set score:', model.evaluate(test_dataset, [metric], transformers))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task: try out the other options for splitting the dataset, which splitting method do you think is the best and why?\n",
    "\n",
    "You may want to run this a few times and think about what the splits mean, do you trust all of the answers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitters = ['random', 'scaffold', 'butina']\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "for splitter in splitters:\n",
    "    tasks, datasets, transformers = dc.molnet.load_tox21(featurizer='ECFP', splitter=splitter)\n",
    "    train_dataset, valid_dataset, test_dataset = datasets\n",
    "    model = dc.models.MultitaskClassifier(n_tasks=len(tasks), n_features=1024, layer_sizes=[1000])\n",
    "    model.fit(train_dataset, nb_epoch=10)\n",
    "    print('splitter:', splitter)\n",
    "    print('training set score:', model.evaluate(train_dataset, [metric], transformers))\n",
    "    print('test set score:', model.evaluate(test_dataset, [metric], transformers))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer*: The scaffold split seems to give the best solution. The random split results must be rejected because a random split can make the problem too easy if the dataset is not balanced (which this isn't) and the fact that it gives vastly better results than the other two methods suggests that the random split results overestimate the goodness of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks, datasets, transformers = dc.molnet.load_tox21(featurizer='GraphConv')\n",
    "train_dataset, valid_dataset, test_dataset = datasets\n",
    "n_tasks = len(tasks)\n",
    "metric = dc.metrics.Metric(dc.metrics.roc_auc_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = dc.models.GraphConvModel(n_tasks, mode='classification')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Useful information: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### To see which datasets are available, run the code below.\n",
    "\n",
    "See also: https://deepchem.readthedocs.io/en/latest/api_reference/moleculenet.html \n",
    "which helpfully tells you the option you can use and the best splitter for that dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[method for method in dir(dc.molnet) if \"load_\" in method ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
