{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8c2bcf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-08 13:42:58.559395: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#Import Statements\n",
    "import deepchem as dc\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers, losses\n",
    "\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import (precision_recall_curve, average_precision_score, PrecisionRecallDisplay)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142edd40",
   "metadata": {},
   "source": [
    "Tox21 Assay List: <br>\n",
    "<ol>\n",
    "    <li> NR-AR </li>\n",
    "    <li> NR-AR-LBD </li>\n",
    "    <li> NR-AhR </li>\n",
    "    <li> NR-Aromatase </li>\n",
    "    <li> NR-ER </li>\n",
    "    <li> NR-ER-LBD </li>\n",
    "    <li> NR-PPAR-gamma </li>\n",
    "    <li> SR-ARE </li>\n",
    "    <li> SR-ATAD5 </li>\n",
    "    <li> SR-HSE </li>\n",
    "    <li> SR-MMP </li>\n",
    "    <li> SR-p53 </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbccf386",
   "metadata": {},
   "outputs": [],
   "source": [
    "assays = ['NR-AR', 'NR-AR-LBD','NR-AhR', 'NR-Aromatase', 'NR-ER', 'NR-ER-LBD', 'NR-PPAR-gamma',\n",
    "         'SR-ARE', 'SR-ATAD5', 'SR-HSE', 'SR-MMP', 'SR-p53']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec35956",
   "metadata": {},
   "source": [
    "## SmilesToImage Featuriser - CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9af0aeff",
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_smiles, datasets_smiles, transformers_smiles = dc.molnet.load_tox21(\n",
    "    featurizer = dc.feat.SmilesToImage(img_size=80, img_spec='std'),\n",
    "    save_dir=r'C:\\Users\\ym20201\\Documents\\Datasets',\n",
    "    data_dir=r'C:\\Users\\ym20201\\Documents\\Datasets')\n",
    "\n",
    "splitter = dc.splits.RandomSplitter()\n",
    "\n",
    "train_data_smiles, valid_data_smiles, test_data_smiles = datasets_smiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b322712d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 17:10:57.278459: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "#SmilesToImage Model 7\n",
    "smiles2img_model = dc.models.CNN(\n",
    "    n_tasks = len(tasks_smiles), # Num of tasks, i.e. width of y\n",
    "    n_features=len(train_data_smiles.X[2]), # number of features, i.e. width of X\n",
    "    dims=1,\n",
    "    layer_filter=[500,500,200],\n",
    "    mode='classification',\n",
    "    weight_init_stddevs=0.02, \n",
    "    bias_init_consts=1.0,\n",
    "    dropouts=0.5,\n",
    "    dense_layer_size=[500,200,100],\n",
    "    activation_fns=['relu'],\n",
    "    uncertainty=False,\n",
    "    pool_type='max',\n",
    "    residual=False,\n",
    "    padding='valid') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ad77a462",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8116668701171875"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smiles2img_model.fit(\n",
    "    train_data_smiles,\n",
    "    nb_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18ff7c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles2img_pred = smiles2img_model.predict(test_data_smiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5978ca73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for accessing y_true and y_pred of each assay\n",
    "def y_true(assay_num, test_data):\n",
    "    y_true = []\n",
    "    for i in range(len(test_data.y)):\n",
    "        y_true.append(test_data.y[i][assay_num - 1])\n",
    "        \n",
    "    return y_true\n",
    "\n",
    "def y_pred(assay_num, pred_data):\n",
    "    y_pred = []\n",
    "    for i in range(len(pred_data)):\n",
    "        y_pred.append( pred_data[i][assay_num - 1][0])\n",
    "#         y_pred = pred_data[i][assay_num - 1] #not sure which one\n",
    "    \n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0916450",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_true of each assay\n",
    "smiles2img_y_true_1 = y_true(1, test_data_smiles)\n",
    "smiles2img_y_true_2 = y_true(2, test_data_smiles)\n",
    "smiles2img_y_true_3 = y_true(3, test_data_smiles)\n",
    "smiles2img_y_true_4 = y_true(4, test_data_smiles)\n",
    "smiles2img_y_true_5 = y_true(5, test_data_smiles)\n",
    "smiles2img_y_true_6 = y_true(6, test_data_smiles)\n",
    "smiles2img_y_true_7 = y_true(7, test_data_smiles)\n",
    "smiles2img_y_true_8 = y_true(8, test_data_smiles)\n",
    "smiles2img_y_true_9 = y_true(9, test_data_smiles)\n",
    "smiles2img_y_true_10 = y_true(10, test_data_smiles)\n",
    "smiles2img_y_true_11 = y_true(11, test_data_smiles)\n",
    "smiles2img_y_true_12 = y_true(12, test_data_smiles)\n",
    "\n",
    "smiles2img_y_true = [smiles2img_y_true_1, smiles2img_y_true_2, smiles2img_y_true_3, smiles2img_y_true_4, smiles2img_y_true_5, \n",
    "        smiles2img_y_true_6, smiles2img_y_true_7, smiles2img_y_true_8, smiles2img_y_true_9, smiles2img_y_true_10,\n",
    "        smiles2img_y_true_11, smiles2img_y_true_12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3441d9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred of each assay\n",
    "smiles2img_y_pred_1 = y_pred(1, smiles2img_pred)\n",
    "smiles2img_y_pred_2 = y_pred(2, smiles2img_pred)\n",
    "smiles2img_y_pred_3 = y_pred(3, smiles2img_pred)\n",
    "smiles2img_y_pred_4 = y_pred(4, smiles2img_pred)\n",
    "smiles2img_y_pred_5 = y_pred(5, smiles2img_pred)\n",
    "smiles2img_y_pred_6 = y_pred(6, smiles2img_pred)\n",
    "smiles2img_y_pred_7 = y_pred(7, smiles2img_pred)\n",
    "smiles2img_y_pred_8 = y_pred(8, smiles2img_pred)\n",
    "smiles2img_y_pred_9 = y_pred(9, smiles2img_pred)\n",
    "smiles2img_y_pred_10 = y_pred(10, smiles2img_pred)\n",
    "smiles2img_y_pred_11 = y_pred(11, smiles2img_pred)\n",
    "smiles2img_y_pred_12 = y_pred(12, smiles2img_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538d1bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_pred(y_pred):\n",
    "    y_pred_new = []\n",
    "    for i in range(len(y_pred)):\n",
    "        if y_pred[i] < 0.5:\n",
    "            new = 0\n",
    "        else:\n",
    "            new = 1\n",
    "        y_pred_new.append(new)\n",
    "    return y_pred_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1e187e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rounding predicted probabilities to binary values\n",
    "smiles2img_y_pred_new_1 = round_pred(smiles2img_y_pred_1)\n",
    "smiles2img_y_pred_new_2 = round_pred(smiles2img_y_pred_2)\n",
    "smiles2img_y_pred_new_3 = round_pred(smiles2img_y_pred_3)\n",
    "smiles2img_y_pred_new_4 = round_pred(smiles2img_y_pred_4)\n",
    "smiles2img_y_pred_new_5 = round_pred(smiles2img_y_pred_5)\n",
    "smiles2img_y_pred_new_6 = round_pred(smiles2img_y_pred_6)\n",
    "smiles2img_y_pred_new_7= round_pred(smiles2img_y_pred_7)\n",
    "smiles2img_y_pred_new_8 = round_pred(smiles2img_y_pred_8)\n",
    "smiles2img_y_pred_new_9 = round_pred(smiles2img_y_pred_9)\n",
    "smiles2img_y_pred_new_10 = round_pred(smiles2img_y_pred_10)\n",
    "smiles2img_y_pred_new_11 = round_pred(smiles2img_y_pred_11)\n",
    "smiles2img_y_pred_new_12 = round_pred(smiles2img_y_pred_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02a31ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the values into arrays\n",
    "smiles2img_y_pred = [smiles2img_y_pred_1, smiles2img_y_pred_2, smiles2img_y_pred_3, smiles2img_y_pred_4, smiles2img_y_pred_5, \n",
    "             smiles2img_y_pred_6, smiles2img_y_pred_7, smiles2img_y_pred_8, smiles2img_y_pred_9, smiles2img_y_pred_10,\n",
    "            smiles2img_y_pred_11, smiles2img_y_pred_12]\n",
    "\n",
    "smiles2img_y_pred_new = [smiles2img_y_pred_new_1, smiles2img_y_pred_new_2, smiles2img_y_pred_new_3, smiles2img_y_pred_new_4, smiles2img_y_pred_new_5, \n",
    "             smiles2img_y_pred_new_6, smiles2img_y_pred_new_7, smiles2img_y_pred_new_8, smiles2img_y_pred_new_9, smiles2img_y_pred_new_10,\n",
    "            smiles2img_y_pred_new_11, smiles2img_y_pred_new_12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66248f55",
   "metadata": {},
   "source": [
    "## ECFP Featuriser - MultitaskClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fe1764",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CircularFingerprint (ECFP) featuriser\n",
    "tasks_ecfp, datasets_ecfp, transformers_ecfp = dc.molnet.load_tox21(\n",
    "    featurizer=dc.feat.CircularFingerprint(),\n",
    "    save_dir=r'C:\\Users\\ym20201\\Documents\\Datasets',\n",
    "    data_dir=r'C:\\Users\\ym20201\\Documents\\Datasets')\n",
    "\n",
    "splitter = dc.splits.RandomSplitter()\n",
    "\n",
    "train_data_ecfp, valid_data_ecfp,test_data_ecfp = datasets_ecfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f4c6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ECFP Model 5\n",
    "ecfp_model = dc.models.RobustMultitaskClassifier(\n",
    "    n_tasks = len(tasks_ecfp),\n",
    "    n_features = len(valid_data_ecfp.X[3]),\n",
    "    layer_sizes=[500,500,200],\n",
    "    weight_init_stddevs=0.02, \n",
    "    bias_init_consts=1.0,\n",
    "    weight_decay_penalty=0.0,\n",
    "    weight_decay_penalty_type='12',\n",
    "    dropouts=[0.8,0.5,0.0],\n",
    "    activation_fns=['relu'],  \n",
    "    n_classes=12,\n",
    "    learning_rate=0.01,\n",
    "    batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b933d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecfp_model.fit(train_data_ecfp,\n",
    "              nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ca8c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecfp_pred = ecfp_model.predict(test_data_ecfp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278dcec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_true of each assay\n",
    "ecfp_y_true_1 = y_true(1, test_data_ecfp)\n",
    "ecfp_y_true_2 = y_true(2, test_data_ecfp)\n",
    "ecfp_y_true_3 = y_true(3, test_data_ecfp)\n",
    "ecfp_y_true_4 = y_true(4, test_data_ecfp)\n",
    "ecfp_y_true_5 = y_true(5, test_data_ecfp)\n",
    "ecfp_y_true_6 = y_true(6, test_data_ecfp)\n",
    "ecfp_y_true_7 = y_true(7, test_data_ecfp)\n",
    "ecfp_y_true_8 = y_true(8, test_data_ecfp)\n",
    "ecfp_y_true_9 = y_true(9, test_data_ecfp)\n",
    "ecfp_y_true_10 = y_true(10, test_data_ecfp)\n",
    "ecfp_y_true_11 = y_true(11, test_data_ecfp)\n",
    "ecfp_y_true_12 = y_true(12, test_data_ecfp)\n",
    "\n",
    "ecfp_y_true = [ecfp_y_true_1, ecfp_y_true_2, ecfp_y_true_3, ecfp_y_true_4, ecfp_y_true_5, \n",
    "        ecfp_y_true_6, ecfp_y_true_7, ecfp_y_true_8, ecfp_y_true_9, ecfp_y_true_10,\n",
    "        ecfp_y_true_11, ecfp_y_true_12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f080854f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred of each assay\n",
    "ecfp_y_pred_1 = y_pred(1, ecfp_pred)\n",
    "ecfp_y_pred_2 = y_pred(2, ecfp_pred)\n",
    "ecfp_y_pred_3 = y_pred(3, ecfp_pred)\n",
    "ecfp_y_pred_4 = y_pred(4, ecfp_pred)\n",
    "ecfp_y_pred_5 = y_pred(5, ecfp_pred)\n",
    "ecfp_y_pred_6 = y_pred(6, ecfp_pred)\n",
    "ecfp_y_pred_7 = y_pred(7, ecfp_pred)\n",
    "ecfp_y_pred_8 = y_pred(8, ecfp_pred)\n",
    "ecfp_y_pred_9 = y_pred(9, ecfp_pred)\n",
    "ecfp_y_pred_10 = y_pred(10, ecfp_pred)\n",
    "ecfp_y_pred_11 = y_pred(11, ecfp_pred)\n",
    "ecfp_y_pred_12 = y_pred(12, ecfp_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd08319d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rounding predicted probabilities to binary values\n",
    "ecfp_y_pred_new_1 = round_pred(ecfp_y_pred_1)\n",
    "ecfp_y_pred_new_2 = round_pred(ecfp_y_pred_2)\n",
    "ecfp_y_pred_new_3 = round_pred(ecfp_y_pred_3)\n",
    "ecfp_y_pred_new_4 = round_pred(ecfp_y_pred_4)\n",
    "ecfp_y_pred_new_5 = round_pred(ecfp_y_pred_5)\n",
    "ecfp_y_pred_new_6 = round_pred(ecfp_y_pred_6)\n",
    "ecfp_y_pred_new_7= round_pred(ecfp_y_pred_7)\n",
    "ecfp_y_pred_new_8 = round_pred(ecfp_y_pred_8)\n",
    "ecfp_y_pred_new_9 = round_pred(ecfp_y_pred_9)\n",
    "ecfp_y_pred_new_10 = round_pred(ecfp_y_pred_10)\n",
    "ecfp_y_pred_new_11 = round_pred(ecfp_y_pred_11)\n",
    "ecfp_y_pred_new_12 = round_pred(ecfp_y_pred_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the values into arrays\n",
    "ecfp_y_pred = [ecfp_y_pred_1, ecfp_y_pred_2, ecfp_y_pred_3, ecfp_y_pred_4, ecfp_y_pred_5, \n",
    "             ecfp_y_pred_6, ecfp_y_pred_7, ecfp_y_pred_8, ecfp_y_pred_9, ecfp_y_pred_10,\n",
    "            ecfp_y_pred_11, ecfp_y_pred_12]\n",
    "\n",
    "ecfp_y_pred_new = [ecfp_y_pred_new_1, ecfp_y_pred_new_2, ecfp_y_pred_new_3, ecfp_y_pred_new_4, ecfp_y_pred_new_5, \n",
    "             ecfp_y_pred_new_6, ecfp_y_pred_new_7, ecfp_y_pred_new_8, ecfp_y_pred_new_9, ecfp_y_pred_new_10,\n",
    "            ecfp_y_pred_new_11, ecfp_y_pred_new_12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587efeb7",
   "metadata": {},
   "source": [
    "## ConvMol Featuriser - GraphConv Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f082cd0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GraphConv featuriser\n",
    "tasks_convmol, datasets_convmol, transformers_convmol = dc.molnet.load_tox21(\n",
    "    featurizer = dc.feat.ConvMolFeaturizer(),\n",
    "    save_dir=r'C:\\Users\\ym20201\\Documents\\Datasets',\n",
    "    data_dir=r'C:\\Users\\ym20201\\Documents\\Datasets')\n",
    "\n",
    "splitter = dc.splits.RandomSplitter()\n",
    "\n",
    "train_data_convmol, valid_data_convmol,test_data_convmol = datasets_convmol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f20486",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 6\n",
    "convmol_model = dc.models.GraphConvModel(\n",
    "    n_tasks = len(tasks_convmol),\n",
    "    graph_conv_layers=[32,32,32],\n",
    "    dense_layer_size=128,\n",
    "    dropout=0.0,\n",
    "    mode='classification',\n",
    "    number_atom_features=75,#default value\n",
    "    batch_normalize=True,\n",
    "    uncertainty=False,\n",
    "    n_classes=12,\n",
    "    learning_rate=0.01,\n",
    "    batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4052ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "convmol_model.fit(\n",
    "    train_data_convmol,\n",
    "    nb_epoch=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb5566a",
   "metadata": {},
   "outputs": [],
   "source": [
    "convmol_pred = convmol_model.predict(test_data_convmol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fff5ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_true of each assay\n",
    "convmol_y_true_1 = y_true(1, test_data_convmol)\n",
    "convmol_y_true_2 = y_true(2, test_data_convmol)\n",
    "convmol_y_true_3 = y_true(3, test_data_convmol)\n",
    "convmol_y_true_4 = y_true(4, test_data_convmol)\n",
    "convmol_y_true_5 = y_true(5, test_data_convmol)\n",
    "convmol_y_true_6 = y_true(6, test_data_convmol)\n",
    "convmol_y_true_7 = y_true(7, test_data_convmol)\n",
    "convmol_y_true_8 = y_true(8, test_data_convmol)\n",
    "convmol_y_true_9 = y_true(9, test_data_convmol)\n",
    "convmol_y_true_10 = y_true(10, test_data_convmol)\n",
    "convmol_y_true_11 = y_true(11, test_data_convmol)\n",
    "convmol_y_true_12 = y_true(12, test_data_convmol)\n",
    "\n",
    "convmol_y_true = [convmol_y_true_1, convmol_y_true_2, convmol_y_true_3, convmol_y_true_4, convmol_y_true_5, \n",
    "        convmol_y_true_6, convmol_y_true_7, convmol_y_true_8, convmol_y_true_9, convmol_y_true_10,\n",
    "        convmol_y_true_11, convmol_y_true_12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a21b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#y_pred of each assay\n",
    "convmol_y_pred_1 = y_pred(1, convmol_pred)\n",
    "convmol_y_pred_2 = y_pred(2, convmol_pred)\n",
    "convmol_y_pred_3 = y_pred(3, convmol_pred)\n",
    "convmol_y_pred_4 = y_pred(4, convmol_pred)\n",
    "convmol_y_pred_5 = y_pred(5, convmol_pred)\n",
    "convmol_y_pred_6 = y_pred(6, convmol_pred)\n",
    "convmol_y_pred_7 = y_pred(7, convmol_pred)\n",
    "convmol_y_pred_8 = y_pred(8, convmol_pred)\n",
    "convmol_y_pred_9 = y_pred(9, convmol_pred)\n",
    "convmol_y_pred_10 = y_pred(10, convmol_pred)\n",
    "convmol_y_pred_11 = y_pred(11, convmol_pred)\n",
    "convmol_y_pred_12 = y_pred(12, convmol_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34fd3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rounding predicted probabilities to binary values\n",
    "convmol_y_pred_new_1 = round_pred(convmol_y_pred_1)\n",
    "convmol_y_pred_new_2 = round_pred(convmol_y_pred_2)\n",
    "convmol_y_pred_new_3 = round_pred(convmol_y_pred_3)\n",
    "convmol_y_pred_new_4 = round_pred(convmol_y_pred_4)\n",
    "convmol_y_pred_new_5 = round_pred(convmol_y_pred_5)\n",
    "convmol_y_pred_new_6 = round_pred(convmol_y_pred_6)\n",
    "convmol_y_pred_new_7= round_pred(convmol_y_pred_7)\n",
    "convmol_y_pred_new_8 = round_pred(convmol_y_pred_8)\n",
    "convmol_y_pred_new_9 = round_pred(convmol_y_pred_9)\n",
    "convmol_y_pred_new_10 = round_pred(convmol_y_pred_10)\n",
    "convmol_y_pred_new_11 = round_pred(convmol_y_pred_11)\n",
    "convmol_y_pred_new_12 = round_pred(convmol_y_pred_12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd003dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the values into arrays\n",
    "convmol_y_pred = [convmol_y_pred_1, convmol_y_pred_2, convmol_y_pred_3, convmol_y_pred_4, convmol_y_pred_5, \n",
    "             convmol_y_pred_6, convmol_y_pred_7, convmol_y_pred_8, convmol_y_pred_9, convmol_y_pred_10,\n",
    "            convmol_y_pred_11, convmol_y_pred_12]\n",
    "\n",
    "convmol_y_pred_new = [convmol_y_pred_new_1, convmol_y_pred_new_2, convmol_y_pred_new_3, convmol_y_pred_new_4, convmol_y_pred_new_5, \n",
    "             convmol_y_pred_new_6, convmol_y_pred_new_7, convmol_y_pred_new_8, convmol_y_pred_new_9, convmol_y_pred_new_10,\n",
    "            convmol_y_pred_new_11, convmol_y_pred_new_12]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b06ffe69",
   "metadata": {},
   "source": [
    "## Balanced Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85e2239",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_accuracy(y_true, y_pred):\n",
    "    balanced_acc = []\n",
    "    for i in range(len(y_true)):\n",
    "        b_acc = balanced_accuracy_score(y_true[i], y_pred[i])\n",
    "        balanced_acc.append(b_acc)\n",
    "    return balanced_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a98f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "smiles2img_balanced_acc = balanced_accuracy(smiles2img_y_true, smiles2img_y_pred_new)\n",
    "ecfp_balanced_acc = balanced_accuracy(ecfp_y_true, ecfp_y_pred_new)\n",
    "convmol_balanced_acc = balanced_accuracy(convmol_y_true, convmol_y_pred_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db98507",
   "metadata": {},
   "outputs": [],
   "source": [
    "means= [np.mean(smiles2img_balanced_acc), np.mean(ecfp_balanced_acc), np.mean(convmol_balanced_acc)]\n",
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2836a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "medians= [np.median(smiles2img_balanced_acc), np.median(ecfp_balanced_acc), np.median(convmol_balanced_acc)]\n",
    "medians"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28598a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouped bar plot\n",
    "fig,ax = plt.subplots(figsize=(22,10))\n",
    "\n",
    "width = 0.2\n",
    "\n",
    "x=np.arange(12)\n",
    "plt.bar(x-0.2, smiles2img_balanced_acc, width, label='SmilesToImage', color='steelblue')\n",
    "plt.bar(x, ecfp_balanced_acc, width, label='ECFP', color='gold')\n",
    "plt.bar(x+0.2, convmol_balanced_acc, width, label='ConvMol', color='indianred')\n",
    "plt.title('Balanced Accuracy Score of Different Featurisers', fontsize=16)\n",
    "plt.ylabel('Balanced Accuracy Score', fontsize=14)\n",
    "plt.xlabel('Tox21 Assays', fontsize=14)\n",
    "\n",
    "#Bar labels\n",
    "plt.bar_label(ax.containers[0], fmt='%.3f', fontsize=12)\n",
    "plt.bar_label(ax.containers[1], fmt='%.3f', fontsize=12)\n",
    "plt.bar_label(ax.containers[2], fmt='%.3f', fontsize=12)\n",
    "\n",
    "#Assay labels\n",
    "plt.xticks(x, assays, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend()\n",
    "# plt.savefig('Overall BA.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f224a014",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouped bar plot - NR\n",
    "fig,ax = plt.subplots(figsize=(22,10))\n",
    "\n",
    "width = 0.2\n",
    "\n",
    "x=np.arange(7)\n",
    "\n",
    "plt.bar(x-0.2, smiles2img_balanced_acc[0:7], width, label='SmilesToImage', color='steelblue')\n",
    "plt.bar(x, ecfp_balanced_acc[0:7], width, label='ECFP', color='gold')\n",
    "plt.bar(x+0.2, convmol_balanced_acc[0:7], width, label='ConvMol', color='indianred')\n",
    "\n",
    "plt.title('Balanced Accuracy Score of Different Featurisers for Nuclear Receptor (NR) Panel', fontsize=16)\n",
    "plt.ylabel('Balanced Accuracy Score', fontsize=14)\n",
    "plt.xlabel('Nuclear Receptor Panel', fontsize=14)\n",
    "\n",
    "#Bar labels\n",
    "plt.bar_label(ax.containers[0], fmt='%.3f', fontsize=12)\n",
    "plt.bar_label(ax.containers[1], fmt='%.3f', fontsize=12)\n",
    "plt.bar_label(ax.containers[2], fmt='%.3f', fontsize=12)\n",
    "\n",
    "#Assay labels\n",
    "plt.xticks(x, assays[0:7],fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend()\n",
    "# plt.savefig('Overall BA-NR.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18c0333",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grouped bar plot - SR\n",
    "fig,ax = plt.subplots(figsize=(20,10))\n",
    "\n",
    "width = 0.2\n",
    "\n",
    "x=np.arange(5)\n",
    "\n",
    "plt.bar(x-0.2, smiles2img_balanced_acc[7:12], width, label='SmilesToImage', color='steelblue')\n",
    "plt.bar(x, ecfp_balanced_acc[7:12], width, label='ECFP', color='gold')\n",
    "plt.bar(x+0.2, convmol_balanced_acc[7:12], width, label='ConvMol', color='indianred')\n",
    "\n",
    "plt.title('Balanced Accuracy Score of Different Featurisers for Stress Response (SR) Panel', fontsize=16)\n",
    "plt.ylabel('Balanced Accuracy Score', fontsize=14)\n",
    "plt.xlabel('Stress Response Panel', fontsize=14)\n",
    "\n",
    "#Bar labels\n",
    "plt.bar_label(ax.containers[0], fmt='%.3f', fontsize=12)\n",
    "plt.bar_label(ax.containers[1], fmt='%.3f', fontsize=12)\n",
    "plt.bar_label(ax.containers[2], fmt='%.3f', fontsize=12)\n",
    "\n",
    "#Assay labels\n",
    "plt.xticks(x, assays[7:12],fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.legend()\n",
    "# plt.savefig('Overall BA-SR.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41930823",
   "metadata": {},
   "source": [
    "From the evaluation of the three models, the CNN model with the SmilesToImage featuriser was observed to have the highest average balanced accuracy score. This model was used to build an autoencoder to improve model performance. \n",
    "\n",
    "Additionally, to investigate the effect of epochs on average balanced accuracy score, the CNN model was run with different epochs, ranging from 1-15. The results were compiled in a separate Excel spreadsheet for evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ffd05c",
   "metadata": {},
   "source": [
    "## Building an autoencoder using the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69e9e098",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting SmilesToImage featurised dataset into smaller train,valid,test datasets\n",
    "splitter = dc.splits.RandomSplitter()\n",
    "\n",
    "train_smiles_small, valid_smiles_small,test_smiles_small = splitter.train_valid_test_split(\n",
    "    datasets_smiles[0], \n",
    "    frac_train = 0.8, frac_valid = 0.08, frac_test = 0.12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b243358d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering out empty arrays in featurised datasets\n",
    "train_feat = []\n",
    "for i in range(len(train_smiles_small.X)):\n",
    "    if train_smiles_small.X[i].shape != (0,):\n",
    "        train_feat.append(train_smiles_small.X[i])\n",
    "        \n",
    "valid_feat = []\n",
    "for i in range(len(valid_smiles_small.X)):\n",
    "    if valid_smiles_small.X[i].shape != (0,):\n",
    "        valid_feat.append(valid_smiles_small.X[i])\n",
    "\n",
    "test_feat = []\n",
    "for i in range(len(test_smiles_small.X)):\n",
    "    if test_smiles_small.X[i].shape != (0,):\n",
    "        test_feat.append(test_smiles_small.X[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdac69d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting numpy array to tensor\n",
    "train_tensor = []\n",
    "for i in range(len(train_feat)):\n",
    "    tensor = tf.convert_to_tensor(train_feat[i])\n",
    "    train_tensor.append(tensor)\n",
    "    \n",
    "valid_tensor = []\n",
    "for i in range(len(valid_feat)):\n",
    "    tensor_v = tf.convert_to_tensor(valid_feat[i])\n",
    "    valid_tensor.append(tensor_v)\n",
    "    \n",
    "test_tensor = []\n",
    "for i in range(len(test_feat)):\n",
    "    tensor_t = tf.convert_to_tensor(test_feat[i])\n",
    "    test_tensor.append(tensor_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8618290d",
   "metadata": {},
   "source": [
    "To start building the autoencoder, the keras blog was used for reference of building a 2D convolutional autoencoder. Firstly, a simple convolutional autoencoder of one convolutional layer was built. The autoencoder was improved by adding more layers such as the max pooling layer and flatten layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dbe2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Simple convolutional autoencoder\n",
    "\n",
    "# # This is the size of our encoded representations\n",
    "# encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# input_img = layers.Input(shape=(40,40,1))\n",
    "\n",
    "# #2D-Convolutional layer\n",
    "# Conv2d_1 = layers.Conv2D(filters=32,\n",
    "#                           kernel_size=3,\n",
    "#                           activation='relu',\n",
    "#                           dilation_rate=2)(input_img)\n",
    "\n",
    "# # \"encoded\" is the encoded representation of the input\n",
    "# encoded = layers.Dense(encoding_dim, activation='relu')(Conv2d_1)\n",
    "\n",
    "# # \"decoded\" is the loss reconstruction of the input\n",
    "# decoded = layers.Dense(784, activation='relu')(encoded)\n",
    "\n",
    "# # This model maps an input to its reconstruction\n",
    "# autoencoder = keras.Model(input_img, decoded)\n",
    "\n",
    "# # This model maps an input to its encoded representation\n",
    "# encoder = keras.Model(input_img, encoded)\n",
    "\n",
    "# # This is our encoded (32-dimensional) input\n",
    "# encoded_input = keras.Input(shape=(encoding_dim,\n",
    "# # Retrieve the last layer of the autoencoder model\n",
    "# decoder_layer = autoencoder.layers[-1]\n",
    "\n",
    "# # Create the decoder model\n",
    "# decoder = keras.Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62abe954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Adding more layers to the convolutional autoencoder\n",
    "\n",
    "# # This is the size of our encoded representations\n",
    "# encoding_dim = 32  # 32 floats -> compression of factor 24.5, assuming the input is 784 floats\n",
    "\n",
    "# input_img = layers.Input(shape=(40,40,1))\n",
    "\n",
    "# #2D-Convolutional layer 1\n",
    "# Conv2D_1 = layers.Conv2D(filters=32,\n",
    "#                           kernel_size=3,\n",
    "#                           activation='relu',\n",
    "#                           dilation_rate=2)(input_img)\n",
    "\n",
    "# #Max pooling layer 1\n",
    "# MaxPool2D_1 = layers.MaxPooling2D((2, 2), padding='same')(Conv2D_1)\n",
    "\n",
    "# #2D-Convolutional layer 2\n",
    "# Conv2D_2 = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(MaxPool2D_1)\n",
    "\n",
    "# #Max pooling layer 2\n",
    "# MaxPool2D_2 = layers.MaxPooling2D((2, 2), padding='same')(Conv2D_2)\n",
    "\n",
    "# #Flatten layer\n",
    "# Flatten = layers.Flatten()(MaxPool2D_2)\n",
    "\n",
    "# encoded = layers.Dense(encoding_dim, activation='relu')(Flatten)\n",
    "\n",
    "\n",
    "# # \"decoded\" is the loss reconstruction of the input\n",
    "# decoded = layers.Dense(784, activation='relu')(encoded)\n",
    "\n",
    "# # This model maps an input to its reconstruction\n",
    "# autoencoder = keras.Model(input_img, decoded)\n",
    "\n",
    "# # This model maps an input to its encoded representation\n",
    "# encoder = keras.Model(input_img, encoded)\n",
    "\n",
    "# # This is our encoded (32-dimensional) input\n",
    "# encoded_input = keras.Input(shape=(encoding_dim,))\n",
    "\n",
    "# # Retrieve the last layer of the autoencoder model\n",
    "# decoder_layer = autoencoder.layers[-1]\n",
    "\n",
    "# # Create the decoder model\n",
    "# decoder = keras.Model(encoded_input, decoder_layer(encoded_input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f7730a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Compiling autoencoder\n",
    "# autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8df333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Training the autoencoder\n",
    "# autoencoder_test = autoencoder.fit(x=train_tensor,\n",
    "#                                    y=train_tensor,\n",
    "#                                    epochs=10,\n",
    "#                                    batch_size=512,\n",
    "#                                    shuffle=True,\n",
    "#                                   validation_data=(valid_tensor, valid_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dbb8af",
   "metadata": {},
   "source": [
    "The above autoencoder model was not able to train on the training data so another method of implementing the keras model was used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9c6c358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Constructing the model\n",
    "\n",
    "# keras_model = Sequential()\n",
    "# keras_model.add(layers.Conv2D(filters=32,\n",
    "#                           kernel_size=3,\n",
    "#                           activation='relu',\n",
    "#                           dilation_rate=2,\n",
    "#                             input_shape=(80, 80, 1)))\n",
    "# keras_model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "# keras_model.add(layers.Dense(50, activation='relu'))\n",
    "# keras_model.add(layers.Dense(1))\n",
    "\n",
    "# keras_model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396b0600",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# keras_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f245c535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras_model.fit(x=train_tensor,\n",
    "#                 y=train_tensor,\n",
    "#                 epochs=10,\n",
    "#                 batch_size=512,\n",
    "#                 shuffle=True,\n",
    "#                validation_data=(valid_tensor, valid_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daba284d",
   "metadata": {},
   "source": [
    "However, this autoencoder also does not work. This could be due to the Tox21 dataset containing multi-dimensional data. To improve on the training of the model, the Tox21 dataset could be split into each of the 12 assays and train the model with each individual assay to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6096ab84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
