{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b7bb192f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Statements\n",
    "import deepchem as dc\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers, losses\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import keras\n",
    "from keras import layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b2dbf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50cab9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #ECFP featuriser - splitting the training dataset into smaller datasets\n",
    "# #THIS WORKS FOR CONV MOL FEAT\n",
    "# tasks, datasets, transformers = dc.molnet.load_tox21(featurizer=dc.feat.ConvMolFeaturizer(),\n",
    "#     splitter='random',\n",
    "#     save_dir=r'C:\\Users\\ym20201\\Documents\\Datasets',\n",
    "#     data_dir=r'C:\\Users\\ym20201\\Documents\\Datasets')\n",
    "\n",
    "# train_data, valid_data, test_data = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "24276622",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ECFP featuriser - splitting the training dataset into smaller datasets\n",
    "tasks, datasets, transformers = dc.molnet.load_tox21(\n",
    "    featurizer=dc.feat.CircularFingerprint(),\n",
    "    save_dir=r'C:\\Users\\ym20201\\Documents\\Datasets',\n",
    "    data_dir=r'C:\\Users\\ym20201\\Documents\\Datasets')\n",
    "\n",
    "# featurizer = dc.feat.CircularFingerprint(size=2048, radius=4)\n",
    "splitter = dc.splits.RandomSplitter()\n",
    "\n",
    "train_data, valid_data,test_data = splitter.train_valid_test_split(\n",
    "    datasets[0], \n",
    "    frac_train = 0.8, frac_valid = 0.08, frac_test = 0.12)\n",
    "\n",
    "# #Featurisation using Featurizer\n",
    "# train_feat = featurizer.featurize(train_data.ids)\n",
    "# valid_feat = featurizer.featurize(valid_data.ids)\n",
    "# test_feat = featurizer.featurize(test_data.ids)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a82f4db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train = tf.convert_to_tensor(train_data.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45eca5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DiskDataset X.shape: (5011, 2048), y.shape: (5011, 12), w.shape: (5011, 12), task_names: ['NR-AR' 'NR-AR-LBD' 'NR-AhR' ... 'SR-HSE' 'SR-MMP' 'SR-p53']>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2ab1325f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DiskDataset X.shape: (501, 2048), y.shape: (501, 12), w.shape: (501, 12), ids: ['O=C(O)c1ccc(O)cc1' 'OCCSSCCO' 'Cc1ccc(S(N)(=O)=O)cc1' ...\n",
       " 'C=C/C(C)=C/C/C=C(\\\\C)CCC=C(C)C' 'CC(=O)Oc1cccc(O)c1'\n",
       " 'CC(C)=CCCC(C)=CC(=O)O'], task_names: ['NR-AR' 'NR-AR-LBD' 'NR-AhR' ... 'SR-HSE' 'SR-MMP' 'SR-p53']>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f41f140",
   "metadata": {},
   "outputs": [],
   "source": [
    "ECFP_model = dc.models.RobustMultitaskClassifier(\n",
    "    n_tasks = len(tasks),\n",
    "    n_features = 5011,\n",
    "    layer_sizes=[500, 200], # how many layers and how many units in each layer to have.\n",
    "    weight_init_stddevs=0.02, \n",
    "    bias_init_consts=1.0,\n",
    "    dropouts=[0.5, 0.0],\n",
    "    activation_fns=['relu', 'relu'],\n",
    "    learning_rate=0.01,\n",
    "    batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57949926",
   "metadata": {},
   "outputs": [],
   "source": [
    "patience =10\n",
    "nb_epochs = 10\n",
    "metric1 = dc.metrics.Metric(dc.metrics.pearson_r2_score)\n",
    "metric2 = dc.metrics.Metric(dc.metrics.mae_score)\n",
    "metrics = [metric1, metric2]\n",
    "\n",
    "callback = dc.models.ValidationCallback(\n",
    "            valid_data,\n",
    "            patience,\n",
    "            metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "242ba822",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\ym20201\\Anaconda3\\envs\\tdaf-tf2p7\\lib\\site-packages\\deepchem\\models\\keras_model.py\", line 464, in apply_gradient_for_batch  *\n        outputs = self.model(inputs, training=True)\n    File \"C:\\Users\\ym20201\\Anaconda3\\envs\\tdaf-tf2p7\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\ym20201\\Anaconda3\\envs\\tdaf-tf2p7\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 5011), found shape=(100, 2048)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2812\\2984490200.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m ECFP_model.fit(train_data,\n\u001b[1;32m----> 2\u001b[1;33m               nb_epoch=10, callbacks=[callback])\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\envs\\tdaf-tf2p7\\lib\\site-packages\\deepchem\\models\\keras_model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, dataset, nb_epoch, max_checkpoints_to_keep, checkpoint_interval, deterministic, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[0;32m    322\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m             deterministic=deterministic), max_checkpoints_to_keep,\n\u001b[1;32m--> 324\u001b[1;33m         checkpoint_interval, restore, variables, loss, callbacks, all_losses)\n\u001b[0m\u001b[0;32m    325\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    326\u001b[0m   def fit_generator(self,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tdaf-tf2p7\\lib\\site-packages\\deepchem\\models\\keras_model.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, max_checkpoints_to_keep, checkpoint_interval, restore, variables, loss, callbacks, all_losses)\u001b[0m\n\u001b[0;32m    407\u001b[0m         \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 409\u001b[1;33m       \u001b[0mbatch_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapply_gradient_for_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    410\u001b[0m       \u001b[0mcurrent_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_global_step\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tdaf-tf2p7\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\tdaf-tf2p7\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1127\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1128\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1129\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1130\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1131\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\ym20201\\Anaconda3\\envs\\tdaf-tf2p7\\lib\\site-packages\\deepchem\\models\\keras_model.py\", line 464, in apply_gradient_for_batch  *\n        outputs = self.model(inputs, training=True)\n    File \"C:\\Users\\ym20201\\Anaconda3\\envs\\tdaf-tf2p7\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 67, in error_handler  **\n        raise e.with_traceback(filtered_tb) from None\n    File \"C:\\Users\\ym20201\\Anaconda3\\envs\\tdaf-tf2p7\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 263, in assert_input_compatibility\n        raise ValueError(f'Input {input_index} of layer \"{layer_name}\" is '\n\n    ValueError: Input 0 of layer \"model\" is incompatible with the layer: expected shape=(None, 5011), found shape=(100, 2048)\n"
     ]
    }
   ],
   "source": [
    "ECFP_model.fit(train_data,\n",
    "              nb_epoch=10, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5782ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = dc.models.GraphConvModel(len(tasks),\n",
    "#                                 batch_size=32,\n",
    "#                                  mode='classification')\n",
    "\n",
    "# # fits the model\n",
    "# model.fit(train_data, nb_epoch=10)\n",
    "# # sets R2 as the metric\n",
    "# metric = dc.metrics.Metric(dc.metrics.roc_auc_score)\n",
    "# #tests and prints out the model\n",
    "# print('training set score:', model.evaluate(train_data, [metric], transformers))\n",
    "# # print('test set score:', model.evaluate(test_data, [metric], transformers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aecc99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbe04d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888bc993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(layers.Conv2D(filters=32,\n",
    "#                           kernel_size=3,\n",
    "#                           activation='relu',\n",
    "#                           dilation_rate=2,\n",
    "#                              input_shape=(80, 80, 1)))\n",
    "# model.add(layers.MaxPooling2D(pool_size=(2,2)))\n",
    "# #dropout to avoid overfitting\n",
    "# model.add(layers.Dense(50, activation='relu'))\n",
    "# model.add(layers.Dense(1))\n",
    "\n",
    "# model.build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78f705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671c4fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(x=train_feat, y=train_feat,\n",
    "#                 epochs=10,\n",
    "#                 batch_size=512,\n",
    "#                 shuffle=True,\n",
    "#                validation_data=(valid_feat,valid_feat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41fffaf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
